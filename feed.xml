<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://christianfang95.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://christianfang95.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-18T16:00:13+00:00</updated><id>https://christianfang95.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website of Christian Fang, statistical researcher at Statistics Netherlands (CBS). </subtitle><entry><title type="html">Learn how to run a basic simulation study in R</title><link href="https://christianfang95.github.io/blog/2023/simulations/" rel="alternate" type="text/html" title="Learn how to run a basic simulation study in R"/><published>2023-03-18T00:00:00+00:00</published><updated>2023-03-18T00:00:00+00:00</updated><id>https://christianfang95.github.io/blog/2023/simulations</id><content type="html" xml:base="https://christianfang95.github.io/blog/2023/simulations/"><![CDATA[<p>You probably know that statistical and machine learning methods are are based on a lot of math. So. much. math. For example, we can prove mathematically that OLS estimates \(\hat\beta\) - provided all of the Gauss-Markov assumptions are met - will in the long run converge to the true population parameters \(\beta\).</p> <p>But what if you do not want to do the math and you still want to test if a given statistical model “works” in a specific situation? Enter simulation studies. In a simulation study, we effectively choose the truth (the so called ground truth) and can test if estimate from our statistical model of choice in the long run converge to that ground truth. We do this by simulating data following from the ground truth we specify, run the model on it, and examine how close the model estimates are to our ground truth. We repeat this process usually thousands of times to get a good idea of how a model behaves “on average” or “in the long run”.</p> <p>This sounds complicated, but is thankfully very straightforward to implement. If you are like me, soon enough you might find yourself running simulations every time you encounter a new statistical model because you just want to see what the heck it does :)</p> <h1 id="basic-workflow-of-a-simulation-study">Basic workflow of a simulation study</h1> <p>The basic workflow of a simulation study is pretty easy.</p> <ol> <li>First, we simulate our data given a ground truth using a random number simulator.</li> <li>Second, run the models we are interested in, on the data we generated in step 1 and store the parameters we are interested in (e.g., regression coefficients, p-values, accuracy…)</li> <li>Third, we repeat this a large number of times (say, 10.000 times).</li> </ol> <p>This is it.</p> <h1 id="a-toy-example-linear-regression-vs-t-test">A toy example: linear regression vs. t-test.</h1> <p>Suppose we are interested in testing who makes more money: data scientist or data engineers. But we are unsure whether we should use a linear regression or an independent sample t-test. We have a hunch that we should use linear regression. In other words: we want to test if a t-test just as good as recovering the ground truth than a linear regression.</p> <h2 id="defining-the-data">Defining the data</h2> <p>For this example, let’s suppose that we want to simulate data based on the following equation:</p> \[income = 50000 + 10000 * data engineer + e(\mu=1, \sigma=0)\] <p>In other words, we assume that the outcome (income) is defined as a linear combination of the intercept term (50), 10 \(\*\) data engineer, and normally distributed error term. This ground truth can be interpreted in the following ways: data scientists on average make 50k a year, and data engineers make 10k more than data scientists. Why do we add an error term here? Several reasons, but the main one is that simulations are supposed to approximate “real life”, and in real life there’s always some part of the variation of the outcome we can’t explain due to completely random processes.</p> <p>We translate this ground truth into the following R code:</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">make_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">sample_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10000</span><span class="p">,</span><span class="w"> 
                          </span><span class="n">intercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">50</span><span class="p">,</span><span class="w"> 
                          </span><span class="n">beta_1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">data_engineer</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rbinom</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sample_size</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">prob</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.5</span><span class="p">)</span><span class="w">
  </span><span class="n">income</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">intercept</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">beta_1</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">data_engineer</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">sample_size</span><span class="p">)</span><span class="w">
  </span><span class="n">data.frame</span><span class="p">(</span><span class="n">income</span><span class="p">,</span><span class="w"> </span><span class="n">data_engineer</span><span class="p">)}</span><span class="w">
</span></code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">rbinom</code> here is used to randomly generate a dummy variable, with the probability of an observation being 0 (=data scientist) or 1 (=data engineer) being .5. This does not mean that in every single data set we use there will be “50% data engineers” and “50% data scientists” (there values might be 51% and 49%), but that this is true on average in the long run. <code class="language-plaintext highlighter-rouge">rnorm</code> simply draws a random number from the normal distribution, and simulates our error term.</p> <h2 id="running-the-models">Running the models</h2> <p>There are many ways of specifying the code for running the models. My preferred way is to do it in a for loop - I’m that basic.</p> <p>Let’s first define the key parameters of the simulation. <code class="language-plaintext highlighter-rouge">nSims</code> defines the number of repetitions (here set to 1000). We specify a random seed using <code class="language-plaintext highlighter-rouge">set.seed</code> so that you get the exact same results as me when running this simulation. We set up two empty containers (<code class="language-plaintext highlighter-rouge">linearreg</code> and <code class="language-plaintext highlighter-rouge">ttest</code>) to store our model estimates.</p> <p>The simulation itself is executed in the for loop. For every iteration in <code class="language-plaintext highlighter-rouge">nSims</code>, we generate a data set using <code class="language-plaintext highlighter-rouge">make_data()</code> we defined above, run a linear regression and store its coefficient in <code class="language-plaintext highlighter-rouge">linearreg</code>, and run a t-test and store the mean difference in <code class="language-plaintext highlighter-rouge">ttest</code>.</p> <p>Grab a cup of coffee while this is running, it might take a while depending on your computer.</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Set number of simulations</span><span class="w">
</span><span class="n">nSims</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1000</span><span class="w">
</span><span class="c1">#Set random seed for reproducibility</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="c1">#Set up vectors for storing results</span><span class="w">
</span><span class="n">linearreg</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">numeric</span><span class="p">(</span><span class="n">nSims</span><span class="p">)</span><span class="w">
</span><span class="n">ttest</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">numeric</span><span class="p">(</span><span class="n">nSims</span><span class="p">)</span><span class="w">

</span><span class="c1">#Run the simulation</span><span class="w">
</span><span class="k">for</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">nSims</span><span class="p">){</span><span class="w">
  </span><span class="c1">#Simulate the data</span><span class="w">
  </span><span class="n">data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">make_data</span><span class="p">()</span><span class="w">
  
  </span><span class="c1">#Estimate linear regression and store coefficient</span><span class="w">
  </span><span class="n">linear</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="s1">'income ~ data_engineer'</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span><span class="w">
  </span><span class="n">linearreg</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">coef</span><span class="p">(</span><span class="n">linear</span><span class="p">)[</span><span class="m">2</span><span class="p">]</span><span class="w">
    
  </span><span class="c1">#Estimate t-test and store result</span><span class="w">
  </span><span class="n">t_test</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t.test</span><span class="p">(</span><span class="n">income</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">data_engineer</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">var.equal</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
  </span><span class="n">ttest</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">(</span><span class="n">t_test</span><span class="p">[[</span><span class="s2">"estimate"</span><span class="p">]][[</span><span class="s2">"mean in group 1"</span><span class="p">]]</span><span class="o">-</span><span class="n">t_test</span><span class="p">[[</span><span class="s2">"estimate"</span><span class="p">]][[</span><span class="s2">"mean in group 0"</span><span class="p">]])</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <h2 id="examining-simulation-results">Examining simulation results</h2> <p>Now that we have our simulation results, it’s time to examine them and draw a conclusion: can we just use a t-test instead of a linear regression when comparing two means (in the specific situation - ground truth - we defined above)?</p> <p>Let’s first examine the means of all estimates from <code class="language-plaintext highlighter-rouge">linearreg</code> and <code class="language-plaintext highlighter-rouge">ttest</code>. If the t-test is as good as the linear regression, its mean difference should be extremely close to the linear regression coefficient, given that we simulated so many data sets.</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mean</span><span class="p">(</span><span class="n">linearreg</span><span class="p">)</span><span class="w">
</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="m">9.999933</span><span class="w">

</span><span class="n">mean</span><span class="p">(</span><span class="n">ttest</span><span class="p">)</span><span class="w">
</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="m">9.999933</span><span class="w">
</span></code></pre></div></div> <p>As we see see, the means are identical. How about the histograms of estimates?</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">hist</span><span class="p">(</span><span class="n">linearreg</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"Histogram of estimates from linear regression"</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="o">=</span><span class="p">(</span><span class="s2">"regression coefficient"</span><span class="p">))</span><span class="w">
</span><span class="n">hist</span><span class="p">(</span><span class="n">ttest</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="s2">"Histogram of estimates from t-test"</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="o">=</span><span class="p">(</span><span class="s2">"mean difference"</span><span class="p">))</span><span class="w">
</span></code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/hist_reg-480.webp 480w, /assets/img/hist_reg-800.webp 800w, /assets/img/hist_reg-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/hist_reg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/hist_ttest-480.webp 480w, /assets/img/hist_ttest-800.webp 800w, /assets/img/hist_ttest-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/hist_ttest.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>They, too, look completely identical and both look like a normal distribution. If we increase the size of <code class="language-plaintext highlighter-rouge">nSims</code>, they histogram will eventually look exactly like a normal distribution.</p> <p>What does this tell us? Yes, if we have a simple model like this (with only an intercept and one dummy variable and normally distributed errors), linear regression and an independent sample t-test recover the exact same mean difference.</p> <h1 id="beyond-simple-examples-what-else-can-you-do-with-simulations">Beyond simple examples: what else can you do with simulations?</h1> <p>This was, of course, only a very simple example meant to illustrate the workflow of a simulation study. You can do so many cool things with simulations. For example, you could examine if it makes a difference whether you use a linear probability model or logistic regression under various circumstances (e.g., correlations between variables, range of variables, etc.). Spoiler alert: it does, the LPM is bad. Or you could test what the role of sample sizes is for tests of statistical significance. Spoiler alert: the bigger the sample size, the smaller the average p-value.</p> <p>If you are more into prediction models, you could test which model yields the highest accuracy/$F_1$: softmax/multinomial logistic regression, KNN, k-means clustering, etc. The possibilities are endless.</p> <h1 id="in-summary">In summary</h1> <p>Simulations help us understand how statistical models behave under different circumstances, and which model might be better in situation X. We can do this entirely without math by simply running our model(s) of interest on simulated data a large number of times (e.g., 10,000 times) and calculate and plot summary statistics of the results.</p>]]></content><author><name></name></author><category term="statistics"/><category term="dataviz"/><category term="R"/><summary type="html"><![CDATA[Simulation studies are extremely valuable tools to understand how statistical models behave. Learn how to run a simulation study comparing linear regression and t-tests in R!]]></summary></entry><entry><title type="html">There are no “marginally significant” p-values</title><link href="https://christianfang95.github.io/blog/2022/marg-sig/" rel="alternate" type="text/html" title="There are no “marginally significant” p-values"/><published>2022-11-25T00:00:00+00:00</published><updated>2022-11-25T00:00:00+00:00</updated><id>https://christianfang95.github.io/blog/2022/marg-sig</id><content type="html" xml:base="https://christianfang95.github.io/blog/2022/marg-sig/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Researchers use \(p\)-values in the context of hypothesis testing to decide whether to accept or reject the null hypothesis. The routine goes as follows: before running the analyis/the hypothesis test, we decide on a significance level (or “alpha level”, typically 5% or 0.05) below which we decide to reject the null hypothesis. Next, we run the analysis/hypothesis test, and compute the \(p\)-value. If the \(p\)-value is below the significance level, we reject the null hypothesis: the \(p\)-value is “statistically significant”. If the \(p\)-value is above the significance level, we “fail to reject” the null hypothesis: the \(p\)-value is not statistically significant.</p> <p>This is illustrated in this nifty pic. If we get a small \(p\)-value we are usually happy (no matter how close it is to 0.05), if we get a big \(p\)-value we cry (and wonder if our paper will ever get published). But what about \(p\)-values that are “hovering” just above 0.05?</p> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/p-value-480.webp 480w, /assets/img/p-value-800.webp 800w, /assets/img/p-value-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/p-value.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In many published papers you will find such a “third kind” of \(p\)-value, which is commonly called the “marginally significant” \(p\)-value. Alternative names include \(p\)-values that are “trending towards significance”, “approaching significance”, “hovering just above the significance level”, or “almost significant”.</p> <p>Such “marginally significant” \(p\)-values, however, simply do not exist. In other words, labelling some \(p\)-values as “marginally significant” is a statistical mistake (which is why I use this term between “ “). Let me tell you why.</p> <h1 id="what-do-people-mean-by-marginally-significant-p-values">What do people mean by “marginally significant” \(p\)-values?</h1> <p>In the null hypothesis significance testing framework, we use the \(p\)-value to decide whether we reject or fail to reject the null. Just like there are only two possible outcomes, there are only two kinds of \(p\)-values: significant ones and non-significant ones. Researchers who claim to have found “marginally significant” \(p\)-values, thus, essentially claim that there is a some (vaguely/subjectively-defined) subset of non-significant \(p\)-values that one could (or perhaps even should!) still interpret as significant.</p> <p>The practice to label some non-significant \(p\)-values as “marginally significant” dates back over 75 years ago. I found a paper published in <em>American Sociological Review</em> in <a href="https://www.jstor.org/stable/2087117?seq=3#metadata_info_tab_contents">1946</a> in which the authors refer to “marginally signficiant” \(p\)-values. So, suffice to say, it’s a mistake that people have been making for a very, very long time, and that has become ingrained in statistical practice in many disciplines.</p> <p>While there is no definition of what a “marginally significant” \(p\)-value is supposed to be (because they do not exist in statistical terms), an empircical analysis of psychology studies found that researchers generally seem to define \(p\)-values between 0.05 and 0.10 as “marginally significant”, though some researchers have apparently labeled \(p\)-values as big as 0.18 as “marginally significant” <a href="https://journals.sagepub.com/doi/abs/10.1177/0956797616645672?casa_token=QvY85WkWqlMAAAAA:taL4m6gL85a2ocmUvtWQtkYbe_GZtvSQzA0jZYppTIlgw58cFKEt_SOcbJJlu2wJos54sTGhf7uCtw">(see this excellent article for more details)</a>. This shows that there is little consistency in what is and isn’t supposed to be “marginally significant” - which makes sense, given that it’s a made-up term that you will for good reason not find in any statistics textbook.</p> <p>As a side note: my suspicion is that researchers speak of “marginally significant” \(p\)-values only if they ran their analysis and the \(p\)-value turned out to be “disappointingly big”. In other words, researchers claim that a non-significant \(p\)-value is “marginally significant”, so that they can prove themselves right (i.e., they can conclude that their study results are in line with their hypothesis). Thus, researchers use their arbitrary gut feeling to after they conducted the analysis(!) decide that some non-significant \(p\)-values are “marginally significant”.</p> <p>Such researchers commit two statistical crimes and a philosophy-of-science crime. The first statistical crime is that these researchers apparently alter their \(\alpha\)-level post-hoc to fit their narrative (a form of confirmation bias and/or \(p\)-hacking). Remember: the \(\alpha\)-level is determined before you run the analysis, and not just changed afterwards. The second statistical crime is that these researchers make a subjective decision which is not backed by statistical theory and invent a whole new (but undefined) class of potential outcomes of a null hypothesis test (i.e., rejecting the null, failing to reject the null, failing to reject but at the same time still somehow rejecting the null).</p> <p>The philosophy-of-science crime is that, per Popper, we are supposed to try to <em>falsify</em> our hypotheses as stringently as possible, not come up with ever-fancier ways of proving ourselves right all the time. This misunderstanding has led to a detrimental culture in which researchers’ hypotheses are confirmed all the time (<a href="https://journals.sagepub.com/doi/abs/10.1177/25152459211007467">over 90% of the time!</a>), partly because many studies are \(p\)-hacked or because the results of hypothesis tests are bent in such ways that they always “confirm” the study hypotheses, even if the data says otherwise. This is dangerous, as it might give students or the next generation of scientists the impression that the point of science is to prove your own preconceived notions to be correct (which is not the point, of course), or that they “made a mistake” if their \(p\)-value is greater than 0.05 (which is nonsense, of course!)</p> <h1 id="why-do-people-claim-to-have-found-marginally-significant-p-values-if-there-is-no-such-thing">Why do people claim to have found “marginally significant” p-values if there is no such thing?</h1> <p>Obviously, I have no idea about what is actually going on in researchers’ heads when they label \(p\)-values as “marginally signficant”, but I have two ideas about where this comes from. The first reason lies in poor education in statistics and a confusion between two competing paradigms for interpreting \(p\)-values. The second reason is the pressure to publish and “be successful” in academia (success as measured by you getting \(p &lt;.05\)), combined with the fact that papers with significant results are more likely to get published than papers that report “null findings”.</p> <h2 id="reason-1-mixing-up-frameworks-for-interpreting-p-values">Reason 1: Mixing up frameworks for interpreting \(p\)-values</h2> <p>Historically, there were two mutually exclusive frameworks to interpret \(p\)-values: Fisher’s and Neyman’s &amp; Pearson’s.</p> <p>Fisher intended for \(p\)-values to be interpreted as a continuous measure of the strength of the evidence against the null hypothesis. Under this framework, a \(p\)-value of 0.20 is viewed as weaker evidence against the null than a \(p\)-value of 0.02.</p> <p>Neyman &amp; Pearson advocated to classify \(p\)-values as either significant or not significant, based on a pre-defined threshold ( \(\alpha\) ). If \(p&lt; \alpha\), we reject the null hypothesis. If \(p &gt; \alpha\), we fail to reject the null hypothesis.</p> <p>Our current way of interpreting \(p\)-values is a mix of the two: on the one hand, we apply a decision rule (like in the Neyman-Pearson framework), on the other hand, we use stars to indicate how small a \(p\)-value is: one star for \(p &lt; .05\), two stars for \(0.05 &lt; p &lt; .01\), and two stars for \(p &lt; .001\). Some R functions, like <code class="language-plaintext highlighter-rouge">lm()</code>, even use a dot ‘.’ for \(p\)-values between 0.1 and 0.05:</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">summary</span><span class="p">(</span><span class="n">lm</span><span class="p">(</span><span class="n">formula</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">x2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">x3</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="p">))</span><span class="w">

</span><span class="n">Coefficients</span><span class="o">:</span><span class="w">
            </span><span class="n">Estimate</span><span class="w"> </span><span class="n">Std.</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="n">Pr</span><span class="p">(</span><span class="o">&gt;|</span><span class="n">t</span><span class="o">|</span><span class="p">)</span><span class="w">    
</span><span class="p">(</span><span class="n">Intercept</span><span class="p">)</span><span class="w">  </span><span class="m">2.95251</span><span class="w">    </span><span class="m">0.03297</span><span class="w">   </span><span class="m">89.55</span><span class="w">   </span><span class="o">&lt;</span><span class="m">2e-16</span><span class="w"> </span><span class="o">***</span><span class="w">
</span><span class="n">x1</span><span class="w">          </span><span class="m">-1.50722</span><span class="w">    </span><span class="m">0.01847</span><span class="w">  </span><span class="m">-81.58</span><span class="w">   </span><span class="o">&lt;</span><span class="m">2e-16</span><span class="w"> </span><span class="o">***</span><span class="w">
</span><span class="n">x2</span><span class="w">           </span><span class="m">0.81344</span><span class="w">    </span><span class="m">0.01808</span><span class="w">   </span><span class="m">44.99</span><span class="w">   </span><span class="o">&lt;</span><span class="m">2e-16</span><span class="w"> </span><span class="o">***</span><span class="w">
</span><span class="n">x3</span><span class="w">          </span><span class="m">-0.82236</span><span class="w">    </span><span class="m">0.03950</span><span class="w">  </span><span class="m">-20.82</span><span class="w">   </span><span class="o">&lt;</span><span class="m">2e-16</span><span class="w"> </span><span class="o">***</span><span class="w">
</span><span class="o">---</span><span class="w">
</span><span class="n">Signif.</span><span class="w"> </span><span class="n">codes</span><span class="o">:</span><span class="w">  </span><span class="m">0</span><span class="w"> </span><span class="err">‘</span><span class="o">***</span><span class="err">’</span><span class="w"> </span><span class="m">0.001</span><span class="w"> </span><span class="err">‘</span><span class="o">**</span><span class="err">’</span><span class="w"> </span><span class="m">0.01</span><span class="w"> </span><span class="err">‘</span><span class="o">*</span><span class="err">’</span><span class="w"> </span><span class="m">0.05</span><span class="w"> </span><span class="err">‘</span><span class="n">.</span><span class="err">’</span><span class="w"> </span><span class="m">0.1</span><span class="w"> </span><span class="err">‘</span><span class="w"> </span><span class="err">’</span><span class="w"> </span><span class="m">1</span><span class="w">

</span></code></pre></div></div> <p>Researchers might be confused about the two paradigms and interpret \(p\)-values in a more Fisherian fashion, and not evaluate them vis-a-vis a decision threshold. Or rather: researchers do acknowledge that \(p\)-values should be below the chosen \(\alpha\)-level in order to be interpreted as significant, but if the \(p\)-value is - in their subjective perception - “close enough” to 0.05, it is “practically” significant, by only a small margin. A.k.a., the \(p\)-value is “marginally significant”.</p> <p>Unfortunately, such an interpretation makes no sense. None of the three frameworks allow for the existence of “marginally significant” \(p\)-values. In Fisher’s framework, there is no hard boundary at which \(p\)-values automatically become “significant”. Per Fisher you would conclude that \(p=0.049\) and \(p=0.051\) constitute practically equivalent evidence against the null, but you would not call either “significant” per se. In Neyman-Pearson and our current way of interpreting \(p\)-values, we are interested in whether \(p&lt; \alpha\). There is no third kind of \(p\)-value that hovers just above our chosen \(\alpha\)-level.</p> <h2 id="reason-2-academic-publish-or-perish-environment-and-researchers-disappointment">Reason 2: Academic publish or perish environment and researchers’ disappointment</h2> <p>Studies in academia take a lot of time to conduct and write up, and oftentimes even longer to get published. Imagine you are a researcher who has spent the last two years (and potentially thousands, if not hundreds of thousands, of euros of tax payers’ money) on a study. You collected the data, ran the analysis, and the \(p\)-value of your hypothesis test is 0.06. The horror! How are you gonna get this published??? You know that journals are way more likely to publish papers that have significant \(p\)-values than papers reporting “null findings”! You were so sure that you were gonna get a significant \(p\)-value!</p> <p>What do you do?</p> <p>The best option is, of course, to report the true outcome: the \(p\)-value is above the chosen \(\alpha\) level, so based on the data that you have, there is insufficient evidence to reject the null. No biggie. Alternatively, you could resort to questionable or downright unethical research practices like \(p\)-hacking. Or you could label your result “marginally significant”, implying that “in the right light, if I had a somewhat bigger sample size, if the stars aligned, this would be significant, I swear!”</p> <p>To be clear, the second option is terrible, don’t do it! The third option is somewhat better, but you should still not do it. For once, you are bending the rules: you shouldn’t willy-nilly change your \(\alpha\)-level after running the analysis just because you don’t like the result (that’s not science). Second, you don’t actually know if you would get a “more significant” \(p\)-value if you had more data/if the sample size were bigger.</p> <h1 id="conclusion">Conclusion</h1> <p>There are no “marginally significant” \(p\)-values, under any framework for interpreting \(p\)-values. Claiming some effect or \(p\)-value is “marginally significant” is wrong: in the null hypothesis testing framework there are just significant and non-significant \(p\)-values, there is not even a definition of what a “marginally significant” \(p\)-value is supposed to be. Using the term “marginally significant” is disingenuous, and honestly renders the whole affair of using a threshold to determine whether or not to reject the null hypothesis pointless. If you are just going to interpret the results any way you want, why use statistics to begin with?</p> <p>Either your \(p\)-value is above or below the a priori chosen significance level. It’s that simple.</p> <p>If you want to read more about “marginally signficant” \(p\)-values, check out these excellent papers:</p> <p>Gibbs, N. M., &amp; Gibbs, S. V. (2015). Misuse of ‘trend’to describe ‘almost significant’ differences in anaesthesia research. <em>British Journal of Anaesthesia</em>, 115(3), 337-339. <a href="https://academic.oup.com/bja/article/115/3/337/312358">Link</a></p> <p>Johnson, V. E. (2019). Evidence from marginally significant t statistics. <em>The American Statistician</em>, 73(sup1), 129-134. <a href="https://www.tandfonline.com/doi/abs/10.1080/00031305.2018.1518788">Link</a></p> <p>Lakens, D. (2021). The practical alternative to the p value is the correctly used p value. <em>Perspectives on psychological science</em>, 16(3), 639-648. <a href="https://journals.sagepub.com/doi/abs/10.1177/1745691620958012">Link</a></p> <p>Pritschet, L., Powell, D., &amp; Horne, Z. (2016). Marginally significant effects as evidence for hypotheses: Changing attitudes over four decades. <em>Psychological science</em>, 27(7), 1036-1042. <a href="https://journals.sagepub.com/doi/abs/10.1177/0956797616645672">Link</a></p>]]></content><author><name></name></author><category term="statistics"/><category term="p-values"/><summary type="html"><![CDATA[Some researchers claim that some non-significant p-values are actually marginally significant. Find out why they're wrong.]]></summary></entry><entry><title type="html">Don’t use linear regression for multiclass classification - Problems with the Multinomial Linear Probability Model</title><link href="https://christianfang95.github.io/blog/2022/mlpm/" rel="alternate" type="text/html" title="Don’t use linear regression for multiclass classification - Problems with the Multinomial Linear Probability Model"/><published>2022-11-17T00:00:00+00:00</published><updated>2022-11-17T00:00:00+00:00</updated><id>https://christianfang95.github.io/blog/2022/mlpm</id><content type="html" xml:base="https://christianfang95.github.io/blog/2022/mlpm/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>In the second edition of “An Introduction to Statistical Learning”, James, Witten, Hastie, and Tibshirani outline why it is a bad idea to use linear regression for a multiclass classification task <a href="https://hastie.su.domains/ISLR2/ISLRv2_website.pdf">(see pg. 131)</a>. In short, the linear regression coefficients and predicted probabilities are dependent on the ordering of the classes.</p> <p>Suppose we want to predict whether someone likes Britney Spears, Taylor Swift, or Cher. If we define the outcome/target variable as follows:</p> \[Y = \begin{cases} 0: Britney \\ 1: Taylor \\ 2: Cher \end{cases}\] <p>and run a linear regression, the coefficients for the predictors mean something completely different compared to if we simply reorder the classes, for example like this:</p> \[Y = \begin{cases} 0: Taylor \\ 1: Cher \\ 2: Britney \end{cases}\] <p>Furthermore, linear regression assumes that the difference between the classes is equally big (which makes absolutely no sense when you talk about a qualitative target variable).</p> <p>On the other hand, James et al. also mention that using linear regression for a <em>binary</em> classification task is generally not all that problematic, though the authors rightly conclude that such an approach is still undesirable for many reasons, for example because you can get nonsensical predicted probabilities such as -0.2 or 1.5. What James et al. don’t mention is that using linear regression on a binary dependent variable <em>is</em> considered a valid approach by (mostly/only) economists and sociologists, who refer to such a model as a “linear probability model” or LPM. I have <a href="https://christianfang95.github.io/posts/2022/10/interactions-logistic/">written about the LPM before</a> and I generally do not think that the LPM makes sense or should be used. However, I hadn’t really thought of using LPMs for classification tasks.</p> <p>That gave me an idea: can’t we simply decompose a multiclass classification problem into a series of LPMs? Would it work, as in: would the prediction performance be at least reasonably high to warrant the use of such a model? After all, estimating a series of LPMs is sort of the same thing as approximating a multinomial logistic regression model by fitting a series of binary logistic regressions.</p> <p>Note: <em>This model is meant to be a joke (literally). As you will see from my conclusion, I do not recommend using the MLPM as a classification algorithm. You should also not use the LPM… Also note that the example of liking either Britney, Taylor, or Cher is completely ridiculous. Of course you should like all of them!</em></p> <p>You can find the code also on <a href="https://github.com/christianfang95/modelsfromscratch/blob/175f2b8f0fc794248b5f2d9947ab99167e29d6d7/mlpm/mlpm.py">GitHub</a>.</p> <h2 id="inventing-anna-the-multinomial-linear-probability-model-mlpm">Inventing <del>Anna</del> the multinomial linear probability model (MLPM)</h2> <p>The “multinomial linear probability model” is not actually a proper statistical model, it is just a series separate “binary” LPMs put together. I am not 100% sure about the mathematical formula for the MLPM (since I just made it up and the model doesn’t really make sense), but I guess it should be something like this (correct me if I’m wrong!):</p> \[Pr(Y_{i} = k) = \alpha + \sum_{j=1}^{k} \beta_{k}X_{i}\] <p>In other words, if we have \(k\) classes to predict, we fit \(k\) separate binary LPMs. For example, if we have three classes, [0, 1, 2] (e.g., liking Taylor, Britney, or Cher), then we would simply fit three LPMs:</p> \[Pr(Y_{i} = 0) = \alpha + \sum \beta_{i}X_{i}\] \[Pr(Y_{i} = 1) = \alpha + \sum \beta_{i}X_{i}\] \[Pr(Y_{i} = 2) = \alpha + \sum \beta_{i}X_{i}\] <p>This gives us three predicted probabilities:</p> <ul> <li>the probability of the observation being in class 0 (as opposed to 1 and 2) (e.g., liking Britney vs. Taylor and Cher)</li> <li>the probability of the observation being in class 1 (as opposed to 0 and 2) (e.g., liking Taylor vs. Britney and Cher)</li> <li>the probability of the observation being in class 2 (as opposed to 0 and 1) (e.g., liking Cher vs. Taylor and Britney)</li> </ul> <p>My intuition told me that this is a deeply weird approach. As described above, the problem with the LPM is that it can yield predicted probabilities outside the interval [0,1]. This is not the case when using a model that explicitly places constraints on the range of the predicted probabilities, such as (multinomial) logistic regression. In case of, for example, multinomial logistic regression, the sum of the predicted probabilities will always be 1 as multinomial logistic regression models a joint probability distribution for all outcomes. In case of the MLPM, just like with the regular “binary” LPM, there are no constraints on the range of the predicted probabilities, which implies that there is the potential for</p> \[\sum_{j=1}^{k} Pr(Y_{i} = k) ≠ 1\] <p>This obviously doesn’t make sense. The sum of predicted probabilities cannot be greater or smaller than 1. I assumed that this fundamental flaw of the “MLPM” would make it a poor classifier.</p> <p>Little did I know…</p> <h1 id="how-does-it-work">(How) does it work?</h1> <p>There are multiple ways of estimating an “MLPM”. One way would be to create dummy variables for the target variable, fit \(k\) LPMs and calculate the \(k\) predicted probabilities.</p> <p>An easier way is to use the <code class="language-plaintext highlighter-rouge">OneVsRestClassifier</code> from <code class="language-plaintext highlighter-rouge">sklearn.multiclass</code>, which breaks down a multiclass classification problem into a series of binary classification tasks. Usually, you would use it with a logistic regression, but we can simply use a linear regression model instead.</p> <p>To assess the extent to which the MLPM “works” as a classification algorithm, I will calculate and plot the F1 scores for all models (indivually per class and averaged). The F1 score is the harmonic mean of precision and recall, and is commonly used to compare the performance of different classifiers. F1 ranges from 0 (bad) to 1 (perfect). So, the higher the F1, the better.</p> <h2 id="importing-packages-and-simulating-data">Importing packages and simulating data</h2> <p>First, let’s import the required packages and define a function that simulates our data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Import required packages
</span><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">sklearn.multiclass</span> <span class="kn">import</span> <span class="n">OneVsRestClassifier</span>
<span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="n">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
</code></pre></div></div> <p>I specify the target to have 3 classes (0: liking Britney, 1: liking Taylor, 2: liking Cher), 1000 observations, and a total of 10 features. Furthermore, I split the data into a training and a testing data set. This is not strictly necessary in our case, but it’s always good to check if our model can generalize to unseen data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">simulate_data</span><span class="p">(</span><span class="n">classes</span> <span class="o">=</span> <span class="mi">3</span><span class="p">):</span>
  <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">make_classification</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> 
                             <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> 
                             <span class="n">n_informative</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> 
                             <span class="n">n_redundant</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span> 
                             <span class="n">n_classes</span> <span class="o">=</span> <span class="n">classes</span><span class="p">,</span> 
                             <span class="n">random_state</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.20</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
  <span class="nf">return</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div> <h2 id="implementing-the-mlpm">Implementing the MLPM</h2> <p>Next, we implement our state-of-the-art MLPM by passing <code class="language-plaintext highlighter-rouge">LinearRegression()</code> from <code class="language-plaintext highlighter-rouge">sklearn.linear_model</code> to the <code class="language-plaintext highlighter-rouge">OneVsRestClassifier</code>. We, then, calculate the predicted proabilities and get the <code class="language-plaintext highlighter-rouge">classification_report</code> as a Pandas data frame. The <code class="language-plaintext highlighter-rouge">classification_report</code> here simply picks the highest predicted probability as the predicted value.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mlpm</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
  <span class="sh">"""</span><span class="s">This function fits multinomial linear probability models on the test data, 
  and gets predictions for the training data
  </span><span class="sh">"""</span>
  <span class="n">lpm</span> <span class="o">=</span> <span class="nc">OneVsRestClassifier</span><span class="p">(</span><span class="nc">LinearRegression</span><span class="p">()).</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">lpm</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
  <span class="n">lpm_report</span> <span class="o">=</span> <span class="nf">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">output_dict</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
  <span class="n">lpm_report</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">lpm_report</span><span class="p">).</span><span class="nf">transpose</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">lpm_report</span>
</code></pre></div></div> <h2 id="implementing-relevant-baselines-multinomial-logistic-regression-and-k-nearest-neighbors">Implementing relevant baselines: multinomial logistic regression and K-nearest neighbors</h2> <p>To gauge the performance of the MLPM, I implemented two “outdated” alternatives to the MLPM (“outdated” as the MLPM is super novel, from 2022 😏), namely multinomial logistic regression and K nearest neighbors. Multinomial logistic regression is an extension of logistic regression (a parametric supervised learning algorithm), and K nearest neighbors (a non-parametric supervised learning method).</p> <p>I defined two functions for multinomial logistic regression and KNN, respecitively:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Create multnomial logistic regression
</span>
<span class="k">def</span> <span class="nf">multinom</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
  <span class="sh">"""</span><span class="s">This function fits multinomial logistic regression on the test data, 
  and gets predictions for the training data
  </span><span class="sh">"""</span>
  <span class="n">multinom</span> <span class="o">=</span> <span class="nc">OneVsRestClassifier</span><span class="p">(</span><span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">multi_class</span> <span class="o">=</span> <span class="sh">"</span><span class="s">multinomial</span><span class="sh">"</span><span class="p">)).</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">multinom</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
  <span class="n">multinom_report</span> <span class="o">=</span> <span class="nf">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">output_dict</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
  <span class="n">multinom_report</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">multinom_report</span><span class="p">).</span><span class="nf">transpose</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">multinom_report</span>

<span class="c1">#Create KNN Classifier
</span>
<span class="k">def</span> <span class="nf">knn</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">classes</span><span class="p">):</span>
  <span class="sh">"""</span><span class="s">This function fits KNN on the test data, 
  and gets predictions for the training data
  </span><span class="sh">"""</span>
  <span class="n">knn</span> <span class="o">=</span> <span class="nc">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="n">classes</span><span class="p">)</span>
  <span class="n">knn</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">knn</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
  <span class="n">knn_report</span> <span class="o">=</span> <span class="nf">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">output_dict</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
  <span class="n">knn_report</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">knn_report</span><span class="p">).</span><span class="nf">transpose</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">knn_report</span>
</code></pre></div></div> <h2 id="plotting-the-results">Plotting the results</h2> <p>Lastly, I define a function to plot the results and wrap all previous functions in a main function:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">lpm</span><span class="p">,</span> <span class="n">log</span><span class="p">,</span> <span class="n">knn</span><span class="p">):</span>
  <span class="sh">"""</span><span class="s">This function plots the F1 scores per class and averaged for all three models</span><span class="sh">"""</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sh">'</span><span class="s">F1 scores of multinomial regressions and KNN</span><span class="sh">'</span><span class="p">)</span>
  
  <span class="c1">#Set line style and line width
</span>  <span class="n">ls</span> <span class="o">=</span> <span class="sh">"</span><span class="s">-</span><span class="sh">"</span>
  <span class="n">lw</span> <span class="o">=</span> <span class="mf">2.5</span>

  <span class="c1">#Add lines for the 3 models
</span>  <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">knn</span><span class="p">.</span><span class="n">index</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">knn</span><span class="p">[</span><span class="sh">'</span><span class="s">f1-score</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="sh">'</span><span class="s">o</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="sh">'</span><span class="s">g</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">linewidth</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="n">ls</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">log</span><span class="p">.</span><span class="n">index</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">log</span><span class="p">[</span><span class="sh">'</span><span class="s">f1-score</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="sh">'</span><span class="s">o</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">linewidth</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="n">ls</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">lpm</span><span class="p">.</span><span class="n">index</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">lpm</span><span class="p">[</span><span class="sh">'</span><span class="s">f1-score</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="sh">'</span><span class="s">o</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">,</span> 
              <span class="n">linewidth</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="n">ls</span><span class="p">)</span>

  
  <span class="c1">#Set axes title, label, and legend
</span>  <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">F1 score</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Class</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">((</span><span class="sh">'</span><span class="s">KNN</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">multinomial </span><span class="se">\n</span><span class="s">logistic regression</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">MLPM</span><span class="sh">'</span><span class="p">))</span>

  <span class="c1">#Plot formatting
</span>  <span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">([</span><span class="sh">'</span><span class="s">0</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">1</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">2</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">],</span> <span class="p">[</span><span class="sh">'</span><span class="s">Britney</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Taylor</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cher</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">])</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
  <span class="sh">"""</span><span class="s">This function calculates the three models and plots the results</span><span class="sh">"""</span>
  <span class="n">classes</span> <span class="o">=</span> <span class="mi">3</span>
  <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">simulate_data</span><span class="p">(</span><span class="n">classes</span> <span class="o">=</span> <span class="n">classes</span><span class="p">)</span>
  <span class="n">mod1</span> <span class="o">=</span> <span class="nf">mlpm</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
  <span class="n">mod2</span> <span class="o">=</span> <span class="nf">multinom</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
  <span class="n">mod3</span> <span class="o">=</span> <span class="nf">knn</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">classes</span> <span class="o">=</span> <span class="n">classes</span><span class="p">)</span>
  <span class="nf">plot</span><span class="p">(</span><span class="n">mod1</span><span class="p">,</span> <span class="n">mod2</span><span class="p">,</span> <span class="n">mod3</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h1 id="the-result-how-well-does-the-mlpm-perform">The result: how well does the MLPM perform?</h1> <p>Now, all there is left to do is to call our main function and examine the resulting plot showing the F1 scores of the three models:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">main</span><span class="p">()</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/mlpm-480.webp 480w, /assets/img/mlpm-800.webp 800w, /assets/img/mlpm-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/mlpm.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The plot shows us that - perhaps shockingly enough - the MLPM does not perform all that terribly. It is only slightly worse than multinomial logistic regression, though when compared to KNN, both the MLPM and multinomial logistic regression don’t perform well.</p> <h1 id="conclusion">Conclusion</h1> <p>Well, I guess the MLPM sort of works for multiclass classification tasks. In hindsight, this is perhaps unsurprising, as multinomial logistic regression also uses a linear predictor function under the hood. The lower F1 scores for the MLPM might be the result of comparatively more incorrect classifications due to nonsensical predicted probabilities.</p> <p>Does this mean you should use the MLPM? I don’t really think so. The MLPM does not make substantive sense, as out-of-range predicted probabilities are just not useful and it is doubtful if any of the parameter estimates make sense or have valid statistical properties. Also, a vanilla multinomial logistic regression (which does not give us problems like nonsense predicted probabilities) performs better, and KNN (and probably many other classification algorithms) beats both. So, why bother?</p>]]></content><author><name></name></author><category term="statistics"/><category term="LPM"/><category term="regression"/><summary type="html"><![CDATA[Learn how to use linear regression for multiclass classification, and why doing sort of works but is not a good idea]]></summary></entry><entry><title type="html">Visualizing Interaction Effects in Logistic Regression and Linear Probability Model</title><link href="https://christianfang95.github.io/blog/2022/interactions/" rel="alternate" type="text/html" title="Visualizing Interaction Effects in Logistic Regression and Linear Probability Model"/><published>2022-10-31T00:00:00+00:00</published><updated>2022-10-31T00:00:00+00:00</updated><id>https://christianfang95.github.io/blog/2022/interactions</id><content type="html" xml:base="https://christianfang95.github.io/blog/2022/interactions/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Many social science studies are interested in investigating interaction effects, or: how the relationship between a predictor (x1) and an outcome (y) depends on a second predictor (x2). x2 is, in this context, sometimes also called a “moderator”, as it moderates or influences the relationship between x1 and x2.</p> <p>Such interaction effects are pretty straightforward in linear regression models, but they tend to be more complicated in a logistic regression model (i.e., when the dependent variable of interest is binary, a.k.a., “zero or one”). There are multiple complications in logistic regression: <a href="http://datacolada.org/57#identifier_5_1354">the sign of the logistic regression coefficient is sometimes misleading</a> (i.e., the coefficient might be negative, but the interaction might be positive for some values of the predictors in the model), and the interaction term in general depends on the values of the variables in the interaction.</p> <p>One solution to making sense of interactions in logistic regression is to use visualizations, a.k.a., plotting the interactions. In this post, I discuss some examples of logistic regression interactions. I consider interactions between:</p> <ul> <li>a dummy variable (0 or 1) and a continuous predictor,</li> <li>a dummy variable and another dummy variable, and</li> <li>a continuous predictor and another continuous predictor.</li> </ul> <p>Additionally, as many people might be tempted to just use linear regression instead for ease of interpretation (i.e., fitting a so-called “linear probability model”), I will illustrate why doing so is usually not a good idea.</p> <p><em>Note: This post is not code-heavy as my other posts, you can check out the code reproducing all plots on my <a href="https://github.com/christianfang95/datascienceprojects/blob/main/simulating-logit-data/interactions_logit_lpm.R">GitHub</a>.</em></p> <h1 id="simulating-the-interactions">Simulating the interactions</h1> <p>I simulated data in R using the canonical approach to simulating logistic regression data. For each scenario, I simulate two predictors (x1 and x2), which are related to the outcome via the logit link:</p> \[P(Y=1|X) = \frac{1}{1 + e^{-(intercept + b_{1}x_{1}+b_{2}x_{2}+b_{3}x_{1}x_{2})}}\] <p>In scenario 1, \(x_{1}\) is a dummy variable and \(x_{2}\) is continuous. In scenario 2, both \(x_{1}\) and \(x_{2}\) are dummy variables. In scenario 3, both \(x_{1}\) and \(x_{2}\) are continuous variables. I simulate 3000 data points in each scenario. I, then, for each scenario, ran a logistic regression and an LPM, and plotted the interaction effects on the probability scale. You can find the R code <a href="https://github.com/christianfang95/datascienceprojects/blob/main/simulating-logit-data/interactions_logit_lpm.R">here</a>.</p> <h2 id="dummy---continuous-interaction">Dummy - Continuous Interaction</h2> <p>Let’s consider an example. We want to estimate how the probability of renting an apartment (as opposed to being a homeowner; y) depends on wealth (x1) and how the effect of wealth (x1) on renting (y) differs between rural and urban areas (x2, where 0=rural, 1=urban).</p> <p>We simulate some data and run two models: a logistic regression model and a linear probability model. Then, we plot the relationship between the predicted probability of renting and two lines for wealth: effect of wealth in rural areas and effect of wealth in urban areas. The two plots are shown below:</p> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/interacton_dummy_cont-480.webp 480w, /assets/img/interacton_dummy_cont-800.webp 800w, /assets/img/interacton_dummy_cont-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/interacton_dummy_cont.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="logistic-regression">Logistic Regression</h3> <p>The first thing we notice about the logistic regression plot is that both lines are nonlinear and S-shaped. This is due to the “logit link” or “logistic transformation” that happens when you fit a logistic regression model. That transformation constrains the predicted probabilities to the [0,1] interval.</p> <p>We see that both in rural and urban regions, more wealth means a lower probability of renting (so, a higher probability of being a homeowner). The relationship between wealth and renting, furthermore, differs a bit between rural and urban regions. In urban regions, the probability of renting is generally higher, but an increase in wealth lowers the probability of renting more substantially than in rural areas. When wealth is about 1.5, the relationship reverses: at high levels of wealth, the probability of renting is higher in rural areas.</p> <p>The S-shaped curve also makes substantive sense: if you are a millionaire already, a, say, €10k increase in wealth is probability not going to reduce your probability of renting as much as when you only have €100k.</p> <h3 id="linear-probability-model-lpm">Linear Probability Model (LPM)</h3> <p>The right part of the figure shows the interaction between wealth and renting in an LPM. As the LPM is just linear regression, it imposes a linear relationship between the independent variables and the dependent variable. Therefore, we see no S-shaped curve here, but two straight lines.</p> <p>Let us first look at the range of the y-axis. The y-axis, which shows the predicted probabilities of renting, ranges from 1.5 to -0.25. Obviously, this doesn’t make any sense: the probability of renting cannot be 150% or -25%. This is the principal, incorrigible, flaw of the LPM. In almost any LPM that contains continuous variables, you will get nonsensical predicted probabilities.</p> <p>But does the LPM interpretation of the interaction at least match that one of the logistic regression model? Not quite. On the whole, LPM does get it sort of right: the relationship between wealth and renting is negative; urban areas have a higher probability of renting than rural areas, but the difference between urban and rural regions shrinks as wealth increases. However, the lines do not cross: LPM apparently does not pick up that the relationship reverses at large values of wealth.</p> <p>I played around a bit with the setup of the simulation, and when I increased the simulated coefficient for the interaction (from -0.5 to -0.8), LPM did eventually show that the relationship reversed, but at a completely different level of wealth than in the logistic regression model (2.2 instead of 1.25).</p> <h2 id="dummy---dummy-interaction">Dummy - Dummy Interaction</h2> <p>When we interact two dummy variables, we want to see how the predicted probabilities of a discrete variable depend on the values of another discrete variable. For example, does the relationship between being unemployed (0=working, 1=unemployed) depend on whether someone lives in a rural or urban region?</p> <p>We, again, run a logistic regression and a linear probability model to investigate this. The plot is given below:</p> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/interaction_dummy_dummy-480.webp 480w, /assets/img/interaction_dummy_dummy-800.webp 800w, /assets/img/interaction_dummy_dummy-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/interaction_dummy_dummy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The interpretation here is straight forward and does not differ between logistic regression and the LPM. Unemployed people have a very high probability of being renters, and the probability of renting is higher in urban than in rural regions. People who are not unemployed have a lower probability of renting, especially those in rural regions.</p> <p>Interestingly enough, the results from the LPM are exactly the same as those from logistic regression, and there do not seem to be any nonsensical predicted probabilities!</p> <h2 id="continuous---continuous-interaction">Continuous - Continuous Interaction</h2> <p>Lastly, let’s see what happens when we interact two continuous variables. Suppose we want to test how the relationship between wealth and renting depends on how happy people are (x2).</p> <p>We, again, run a logistic regression and an LPM:</p> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/interaction_cont_cont-480.webp 480w, /assets/img/interaction_cont_cont-800.webp 800w, /assets/img/interaction_cont_cont-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/interaction_cont_cont.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="logistic-regression-1">Logistic Regression</h3> <p>The left part of the plot shows that, in logistic regression, the relationship between wealth and renting is, in general, negative: the wealthier someone is, the less likely that someone will be a renter. This relationship depends, however, on how happy someone is. Up until a value of about “1” on wealth, happier people have a higher probability of renting than people who are less happy. At a value of “1” on wealth, the relationship reverses: now, happy people have a lower probability of renting than sad people.</p> <h3 id="linear-probability-model">Linear Probability Model</h3> <p>The LPM gets the overall pattern somewhat OK: the gap between the probabilities of happy and less happy people being renters decreases as wealth increases. Unlike logistic regression, the LPM fails to show that the relationship reverses at a value of about “1” on wealth. Instead, the LPM shows the lines to converge around 4.8 on “wealth”. Furthermore, the range of predicted probabilities makes no sense: the y-axis ranges from a bit more than 2(!) to about -0.4. This corresponds to a 200% or -40% probability of renting. Yikes!</p> <h1 id="conclusions">Conclusions</h1> <p>What can we learn from all of this?</p> <ol> <li> <p>Interaction effects involving continuous variables are always non-linear in logistic regression (the same goes for main effects). This is due to the model itself: logistic regression is based on a logit transformation of the linear predictor, which keeps the range of the predicted probabilities confined to between zero and one. This means that the coefficient of the interaction might, for example, be negative, but the interaction is likely to be positive for some value of \(x_{1}\) or \(x_{2}\). You pretty much only see this if you plot the interaction.</p> </li> <li> <p>Interactions in the linear probability model appears to be a good approximation of interactions in logistic regression <em>as long as the variables involved are dummy variables</em>. As soon as a continuous variable is involved in the interaction, LPM interactions can deviate more or less substantially from their logistic counterpart. LPM starts to produce nonsensical predicted probabilities, and the “meaning” of the interaction changes. For example, logistic regression might suggest that the interaction reverses at a certain point, whereas LPM might just show you that differences converge, but do not reverse.</p> </li> <li> <p>Bonus: If you play around with the simulation, you will see that a tiny change in one of the simulated coefficients might produce astonishingly big changes in the LPM interactions. In that sense, the LPM seems pretty inconsistent.</p> </li> </ol> <p>What are the implications of 1.), 2.), and 3.)?</p> <ol> <li> <p>Be careful when interpreting interaction effects in logistic regression models. The coefficients often do not tell you the whole story.</p> </li> <li> <p>The LPM is defensible when the model contains only dummy variables, but seems inappropriate when the model/the interaction involves continuous variables. You are very likely to get nonsensical predicted probabilities.</p> </li> <li> <p>Use logistic regression instead of the LPM. It is closer to the truth and can be more realistically interpreted.</p> </li> </ol>]]></content><author><name></name></author><category term="statistics"/><category term="dataviz"/><category term="R"/><summary type="html"><![CDATA[Learn how to interpret plots of interactions in logistic regression and why using LPM instead is a bad idea]]></summary></entry></feed>