<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="introduction">Introduction</h1> <p>In the second edition of “An Introduction to Statistical Learning”, James, Witten, Hastie, and Tibshirani outline why it is a bad idea to use linear regression for a multiclass classification task <a href="https://hastie.su.domains/ISLR2/ISLRv2_website.pdf" rel="external nofollow noopener" target="_blank">(see pg. 131)</a>. In short, the linear regression coefficients and predicted probabilities are dependent on the ordering of the classes.</p> <p>Suppose we want to predict whether someone likes Britney Spears, Taylor Swift, or Cher. If we define the outcome/target variable as follows:</p> \[Y = \begin{cases} 0: Britney \\ 1: Taylor \\ 2: Cher \end{cases}\] <p>and run a linear regression, the coefficients for the predictors mean something completely different compared to if we simply reorder the classes, for example like this:</p> \[Y = \begin{cases} 0: Taylor \\ 1: Cher \\ 2: Britney \end{cases}\] <p>Furthermore, linear regression assumes that the difference between the classes is equally big (which makes absolutely no sense when you talk about a qualitative target variable).</p> <p>On the other hand, James et al. also mention that using linear regression for a <em>binary</em> classification task is generally not all that problematic, though the authors rightly conclude that such an approach is still undesirable for many reasons, for example because you can get nonsensical predicted probabilities such as -0.2 or 1.5. What James et al. don’t mention is that using linear regression on a binary dependent variable <em>is</em> considered a valid approach by (mostly/only) economists and sociologists, who refer to such a model as a “linear probability model” or LPM. I have <a href="https://christianfang95.github.io/posts/2022/10/interactions-logistic/">written about the LPM before</a> and I generally do not think that the LPM makes sense or should be used. However, I hadn’t really thought of using LPMs for classification tasks.</p> <p>That gave me an idea: can’t we simply decompose a multiclass classification problem into a series of LPMs? Would it work, as in: would the prediction performance be at least reasonably high to warrant the use of such a model? After all, estimating a series of LPMs is sort of the same thing as approximating a multinomial logistic regression model by fitting a series of binary logistic regressions.</p> <p>Note: <em>This model is meant to be a joke (literally). As you will see from my conclusion, I do not recommend using the MLPM as a classification algorithm. You should also not use the LPM… Also note that the example of liking either Britney, Taylor, or Cher is completely ridiculous. Of course you should like all of them!</em></p> <p>You can find the code also on <a href="https://github.com/christianfang95/modelsfromscratch/blob/175f2b8f0fc794248b5f2d9947ab99167e29d6d7/mlpm/mlpm.py" rel="external nofollow noopener" target="_blank">GitHub</a>.</p> <h2 id="inventing-anna-the-multinomial-linear-probability-model-mlpm">Inventing <del>Anna</del> the multinomial linear probability model (MLPM)</h2> <p>The “multinomial linear probability model” is not actually a proper statistical model, it is just a series separate “binary” LPMs put together. I am not 100% sure about the mathematical formula for the MLPM (since I just made it up and the model doesn’t really make sense), but I guess it should be something like this (correct me if I’m wrong!):</p> \[Pr(Y_{i} = k) = \alpha + \sum_{j=1}^{k} \beta_{k}X_{i}\] <p>In other words, if we have $k$ classes to predict, we fit $k$ separate binary LPMs. For example, if we have three classes, [0, 1, 2] (e.g., liking Taylor, Britney, or Cher), then we would simply fit three LPMs:</p> \[Pr(Y_{i} = 0) = \alpha + \sum \beta_{i}X_{i}\] \[Pr(Y_{i} = 1) = \alpha + \sum \beta_{i}X_{i}\] \[Pr(Y_{i} = 2) = \alpha + \sum \beta_{i}X_{i}\] <p>This gives us three predicted probabilities:</p> <ul> <li>the probability of the observation being in class 0 (as opposed to 1 and 2) (e.g., liking Britney vs. Taylor and Cher)</li> <li>the probability of the observation being in class 1 (as opposed to 0 and 2) (e.g., liking Taylor vs. Britney and Cher)</li> <li>the probability of the observation being in class 2 (as opposed to 0 and 1) (e.g., liking Cher vs. Taylor and Britney)</li> </ul> <p>My intuition told me that this is a deeply weird approach. As described above, the problem with the LPM is that it can yield predicted probabilities outside the interval [0,1]. This is not the case when using a model that explicitly places constraints on the range of the predicted probabilities, such as (multinomial) logistic regression. In case of, for example, multinomial logistic regression, the sum of the predicted probabilities will always be 1 as multinomial logistic regression models a joint probability distribution for all outcomes. In case of the MLPM, just like with the regular “binary” LPM, there are no constraints on the range of the predicted probabilities, which implies that there is the potential for</p> \[\sum_{j=1}^{k} Pr(Y_{i} = k) ≠ 1\] <p>This obviously doesn’t make sense. The sum of predicted probabilities cannot be greater or smaller than 1. I assumed that this fundamental flaw of the “MLPM” would make it a poor classifier.</p> <p>Little did I know…</p> <h1 id="how-does-it-work">(How) does it work?</h1> <p>There are multiple ways of estimating an “MLPM”. One way would be to create dummy variables for the target variable, fit $k$ LPMs and calculate the $k$ predicted probabilities.</p> <p>An easier way is to use the <code class="language-plaintext highlighter-rouge">OneVsRestClassifier</code> from <code class="language-plaintext highlighter-rouge">sklearn.multiclass</code>, which breaks down a multiclass classification problem into a series of binary classification tasks. Usually, you would use it with a logistic regression, but we can simply use a linear regression model instead.</p> <p>To assess the extent to which the MLPM “works” as a classification algorithm, I will calculate and plot the F1 scores for all models (indivually per class and averaged). The F1 score is the harmonic mean of precision and recall, and is commonly used to compare the performance of different classifiers. F1 ranges from 0 (bad) to 1 (perfect). So, the higher the F1, the better.</p> <h2 id="importing-packages-and-simulating-data">Importing packages and simulating data</h2> <p>First, let’s import the required packages and define a function that simulates our data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Import required packages
</span><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">sklearn.multiclass</span> <span class="kn">import</span> <span class="n">OneVsRestClassifier</span>
<span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="n">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
</code></pre></div></div> <p>I specify the target to have 3 classes (0: liking Britney, 1: liking Taylor, 2: liking Cher), 1000 observations, and a total of 10 features. Furthermore, I split the data into a training and a testing data set. This is not strictly necessary in our case, but it’s always good to check if our model can generalize to unseen data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">simulate_data</span><span class="p">(</span><span class="n">classes</span> <span class="o">=</span> <span class="mi">3</span><span class="p">):</span>
  <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">make_classification</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> 
                             <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> 
                             <span class="n">n_informative</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> 
                             <span class="n">n_redundant</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span> 
                             <span class="n">n_classes</span> <span class="o">=</span> <span class="n">classes</span><span class="p">,</span> 
                             <span class="n">random_state</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.20</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
  <span class="nf">return</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div> <h2 id="implementing-the-mlpm">Implementing the MLPM</h2> <p>Next, we implement our state-of-the-art MLPM by passing <code class="language-plaintext highlighter-rouge">LinearRegression()</code> from <code class="language-plaintext highlighter-rouge">sklearn.linear_model</code> to the <code class="language-plaintext highlighter-rouge">OneVsRestClassifier</code>. We, then, calculate the predicted proabilities and get the <code class="language-plaintext highlighter-rouge">classification_report</code> as a Pandas data frame. The <code class="language-plaintext highlighter-rouge">classification_report</code> here simply picks the highest predicted probability as the predicted value.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mlpm</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
  <span class="sh">"""</span><span class="s">This function fits multinomial linear probability models on the test data, 
  and gets predictions for the training data
  </span><span class="sh">"""</span>
  <span class="n">lpm</span> <span class="o">=</span> <span class="nc">OneVsRestClassifier</span><span class="p">(</span><span class="nc">LinearRegression</span><span class="p">()).</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">lpm</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
  <span class="n">lpm_report</span> <span class="o">=</span> <span class="nf">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">output_dict</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
  <span class="n">lpm_report</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">lpm_report</span><span class="p">).</span><span class="nf">transpose</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">lpm_report</span>
</code></pre></div></div> <h2 id="implementing-relevant-baselines-multinomial-logistic-regression-and-k-nearest-neighbors">Implementing relevant baselines: multinomial logistic regression and K-nearest neighbors</h2> <p>To gauge the performance of the MLPM, I implemented two “outdated” alternatives to the MLPM (“outdated” as the MLPM is super novel, from 2022 😏), namely multinomial logistic regression and K nearest neighbors. Multinomial logistic regression is an extension of logistic regression (a parametric supervised learning algorithm), and K nearest neighbors (a non-parametric supervised learning method).</p> <p>I defined two functions for multinomial logistic regression and KNN, respecitively:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Create multnomial logistic regression
</span>
<span class="k">def</span> <span class="nf">multinom</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
  <span class="sh">"""</span><span class="s">This function fits multinomial logistic regression on the test data, 
  and gets predictions for the training data
  </span><span class="sh">"""</span>
  <span class="n">multinom</span> <span class="o">=</span> <span class="nc">OneVsRestClassifier</span><span class="p">(</span><span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">multi_class</span> <span class="o">=</span> <span class="sh">"</span><span class="s">multinomial</span><span class="sh">"</span><span class="p">)).</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">multinom</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
  <span class="n">multinom_report</span> <span class="o">=</span> <span class="nf">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">output_dict</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
  <span class="n">multinom_report</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">multinom_report</span><span class="p">).</span><span class="nf">transpose</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">multinom_report</span>

<span class="c1">#Create KNN Classifier
</span>
<span class="k">def</span> <span class="nf">knn</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">classes</span><span class="p">):</span>
  <span class="sh">"""</span><span class="s">This function fits KNN on the test data, 
  and gets predictions for the training data
  </span><span class="sh">"""</span>
  <span class="n">knn</span> <span class="o">=</span> <span class="nc">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="n">classes</span><span class="p">)</span>
  <span class="n">knn</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">knn</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
  <span class="n">knn_report</span> <span class="o">=</span> <span class="nf">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">output_dict</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
  <span class="n">knn_report</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">knn_report</span><span class="p">).</span><span class="nf">transpose</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">knn_report</span>
</code></pre></div></div> <h2 id="plotting-the-results">Plotting the results</h2> <p>Lastly, I define a function to plot the results and wrap all previous functions in a main function:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">lpm</span><span class="p">,</span> <span class="n">log</span><span class="p">,</span> <span class="n">knn</span><span class="p">):</span>
  <span class="sh">"""</span><span class="s">This function plots the F1 scores per class and averaged for all three models</span><span class="sh">"""</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sh">'</span><span class="s">F1 scores of multinomial regressions and KNN</span><span class="sh">'</span><span class="p">)</span>
  
  <span class="c1">#Set line style and line width
</span>  <span class="n">ls</span> <span class="o">=</span> <span class="sh">"</span><span class="s">-</span><span class="sh">"</span>
  <span class="n">lw</span> <span class="o">=</span> <span class="mf">2.5</span>

  <span class="c1">#Add lines for the 3 models
</span>  <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">knn</span><span class="p">.</span><span class="n">index</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">knn</span><span class="p">[</span><span class="sh">'</span><span class="s">f1-score</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="sh">'</span><span class="s">o</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="sh">'</span><span class="s">g</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">linewidth</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="n">ls</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">log</span><span class="p">.</span><span class="n">index</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">log</span><span class="p">[</span><span class="sh">'</span><span class="s">f1-score</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="sh">'</span><span class="s">o</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">linewidth</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="n">ls</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">lpm</span><span class="p">.</span><span class="n">index</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">lpm</span><span class="p">[</span><span class="sh">'</span><span class="s">f1-score</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="sh">'</span><span class="s">o</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">,</span> 
              <span class="n">linewidth</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="n">ls</span><span class="p">)</span>

  
  <span class="c1">#Set axes title, label, and legend
</span>  <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">F1 score</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Class</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">((</span><span class="sh">'</span><span class="s">KNN</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">multinomial </span><span class="se">\n</span><span class="s">logistic regression</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">MLPM</span><span class="sh">'</span><span class="p">))</span>

  <span class="c1">#Plot formatting
</span>  <span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">([</span><span class="sh">'</span><span class="s">0</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">1</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">2</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">],</span> <span class="p">[</span><span class="sh">'</span><span class="s">Britney</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Taylor</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cher</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">])</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
  <span class="sh">"""</span><span class="s">This function calculates the three models and plots the results</span><span class="sh">"""</span>
  <span class="n">classes</span> <span class="o">=</span> <span class="mi">3</span>
  <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">simulate_data</span><span class="p">(</span><span class="n">classes</span> <span class="o">=</span> <span class="n">classes</span><span class="p">)</span>
  <span class="n">mod1</span> <span class="o">=</span> <span class="nf">mlpm</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
  <span class="n">mod2</span> <span class="o">=</span> <span class="nf">multinom</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
  <span class="n">mod3</span> <span class="o">=</span> <span class="nf">knn</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">classes</span> <span class="o">=</span> <span class="n">classes</span><span class="p">)</span>
  <span class="nf">plot</span><span class="p">(</span><span class="n">mod1</span><span class="p">,</span> <span class="n">mod2</span><span class="p">,</span> <span class="n">mod3</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h1 id="the-result-how-well-does-the-mlpm-perform">The result: how well does the MLPM perform?</h1> <p>Now, all there is left to do is to call our main function and examine the resulting plot showing the F1 scores of the three models:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">main</span><span class="p">()</span>
</code></pre></div></div> <p>The plot shows us that - perhaps shockingly enough - the MLPM does not perform all that terribly. It is only slightly worse than multinomial logistic regression, though when compared to KNN, both the MLPM and multinomial logistic regression don’t perform well.</p> <h1 id="conclusion">Conclusion</h1> <p>Well, I guess the MLPM sort of works for multiclass classification tasks. In hindsight, this is perhaps unsurprising, as multinomial logistic regression also uses a linear predictor function under the hood. The lower F1 scores for the MLPM might be the result of comparatively more incorrect classifications due to nonsensical predicted probabilities.</p> <p>Does this mean you should use the MLPM? I don’t really think so. The MLPM does not make substantive sense, as out-of-range predicted probabilities are just not useful and it is doubtful if any of the parameter estimates make sense or have valid statistical properties. Also, a vanilla multinomial logistic regression (which does not give us problems like nonsense predicted probabilities) performs better, and KNN (and probably many other classification algorithms) beats both. So, why bother?</p> </body></html>