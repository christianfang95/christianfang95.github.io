[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Learn how to run a basic simulation study in R\n\n\n\nstatistics\n\ndataviz\n\nR\n\n\n\nSimulation studies are extremely valuable tools to understand how statistical models behave. Learn how to run a simulation study comparing linear regression and t-tests in R!\n\n\n\n\n\nMar 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThere are no “marginally significant” p-values\n\n\n\nstatistics\n\np-values\n\n\n\nSome researchers claim that some non-significant p-values are actually “marginally significant”. Find out why they’re wrong.\n\n\n\n\n\nNov 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t use linear regression for multiclass classification - Problems with the Multinomial Linear Probability Model\n\n\n\nstatistics\n\nLPM\n\nregression\n\nPython\n\n\n\nLearn how to use linear regression for multiclass classification, and why doing sort of works but is not a good idea.\n\n\n\n\n\nNov 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Interaction Effects in Logistic Regression and Linear Probability Model\n\n\n\nstatistics\n\ndataviz\n\nR\n\n\n\nLearn how to interpret plots of interactions in logistic regression and why using LPM instead is a bad idea.\n\n\n\n\n\nOct 31, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/mlpm/index.html",
    "href": "posts/mlpm/index.html",
    "title": "Don’t use linear regression for multiclass classification - Problems with the Multinomial Linear Probability Model",
    "section": "",
    "text": "In the second edition of “An Introduction to Statistical Learning”, James, Witten, Hastie, and Tibshirani outline why it is a bad idea to use linear regression for a multiclass classification task (see pg. 131). In short, the linear regression coefficients and predicted probabilities are dependent on the ordering of the classes.\nSuppose we want to predict whether someone likes Britney Spears, Taylor Swift, or Cher. If we define the outcome/target variable as follows:\n  Y =\n  \\begin{cases}\n    0: Britney \\\\\n    1: Taylor \\\\\n    2: Cher\n  \\end{cases} \nand run a linear regression, the coefficients for the predictors mean something completely different compared to if we simply reorder the classes, for example like this:\n  Y =\n  \\begin{cases}\n    0: Taylor \\\\\n    1: Cher \\\\\n    2: Britney\n  \\end{cases} \nFurthermore, linear regression assumes that the difference between the classes is equally big (which makes absolutely no sense when you talk about a qualitative target variable).\nOn the other hand, James et al. also mention that using linear regression for a binary classification task is generally not all that problematic, though the authors rightly conclude that such an approach is still undesirable for many reasons, for example because you can get nonsensical predicted probabilities such as -0.2 or 1.5. What James et al. don’t mention is that using linear regression on a binary dependent variable is considered a valid approach by (mostly/only) economists and sociologists, who refer to such a model as a “linear probability model” or LPM. I have written about the LPM before and I generally do not think that the LPM makes sense or should be used. However, I hadn’t really thought of using LPMs for classification tasks.\nThat gave me an idea: can’t we simply decompose a multiclass classification problem into a series of LPMs? Would it work, as in: would the prediction performance be at least reasonably high to warrant the use of such a model? After all, estimating a series of LPMs is sort of the same thing as approximating a multinomial logistic regression model by fitting a series of binary logistic regressions.\nNote: This model is meant to be a joke (literally). As you will see from my conclusion, I do not recommend using the MLPM as a classification algorithm. You should also not use the LPM… Also note that the example of liking either Britney, Taylor, or Cher is completely ridiculous. Of course you should like all of them!\nYou can find the code also on GitHub.\n\n\nThe “multinomial linear probability model” is not actually a proper statistical model, it is just a series separate “binary” LPMs put together. I am not 100% sure about the mathematical formula for the MLPM (since I just made it up and the model doesn’t really make sense), but I guess it should be something like this (correct me if I’m wrong!):\nPr(Y_{i} = k) = \\alpha + \\sum_{j=1}^{k} \\beta_{k}X_{i}\nIn other words, if we have k classes to predict, we fit k separate binary LPMs. For example, if we have three classes, [0, 1, 2] (e.g., liking Taylor, Britney, or Cher), then we would simply fit three LPMs:\nPr(Y_{i} = 0) = \\alpha + \\sum \\beta_{i}X_{i}\nPr(Y_{i} = 1) = \\alpha + \\sum \\beta_{i}X_{i}\nPr(Y_{i} = 2) = \\alpha + \\sum \\beta_{i}X_{i}\nThis gives us three predicted probabilities:\n\nthe probability of the observation being in class 0 (as opposed to 1 and 2) (e.g., liking Britney vs. Taylor and Cher)\nthe probability of the observation being in class 1 (as opposed to 0 and 2) (e.g., liking Taylor vs. Britney and Cher)\nthe probability of the observation being in class 2 (as opposed to 0 and 1) (e.g., liking Cher vs. Taylor and Britney)\n\nMy intuition told me that this is a deeply weird approach. As described above, the problem with the LPM is that it can yield predicted probabilities outside the interval [0,1]. This is not the case when using a model that explicitly places constraints on the range of the predicted probabilities, such as (multinomial) logistic regression. In case of, for example, multinomial logistic regression, the sum of the predicted probabilities will always be 1 as multinomial logistic regression models a joint probability distribution for all outcomes. In case of the MLPM, just like with the regular “binary” LPM, there are no constraints on the range of the predicted probabilities, which implies that there is the potential for\n\\sum_{j=1}^{k} Pr(Y_{i} = k) ≠ 1\nThis obviously doesn’t make sense. The sum of predicted probabilities cannot be greater or smaller than 1. I assumed that this fundamental flaw of the “MLPM” would make it a poor classifier.\nLittle did I know…"
  },
  {
    "objectID": "posts/mlpm/index.html#inventing-anna-the-multinomial-linear-probability-model-mlpm",
    "href": "posts/mlpm/index.html#inventing-anna-the-multinomial-linear-probability-model-mlpm",
    "title": "Don’t use linear regression for multiclass classification - Problems with the Multinomial Linear Probability Model",
    "section": "",
    "text": "The “multinomial linear probability model” is not actually a proper statistical model, it is just a series separate “binary” LPMs put together. I am not 100% sure about the mathematical formula for the MLPM (since I just made it up and the model doesn’t really make sense), but I guess it should be something like this (correct me if I’m wrong!):\nPr(Y_{i} = k) = \\alpha + \\sum_{j=1}^{k} \\beta_{k}X_{i}\nIn other words, if we have k classes to predict, we fit k separate binary LPMs. For example, if we have three classes, [0, 1, 2] (e.g., liking Taylor, Britney, or Cher), then we would simply fit three LPMs:\nPr(Y_{i} = 0) = \\alpha + \\sum \\beta_{i}X_{i}\nPr(Y_{i} = 1) = \\alpha + \\sum \\beta_{i}X_{i}\nPr(Y_{i} = 2) = \\alpha + \\sum \\beta_{i}X_{i}\nThis gives us three predicted probabilities:\n\nthe probability of the observation being in class 0 (as opposed to 1 and 2) (e.g., liking Britney vs. Taylor and Cher)\nthe probability of the observation being in class 1 (as opposed to 0 and 2) (e.g., liking Taylor vs. Britney and Cher)\nthe probability of the observation being in class 2 (as opposed to 0 and 1) (e.g., liking Cher vs. Taylor and Britney)\n\nMy intuition told me that this is a deeply weird approach. As described above, the problem with the LPM is that it can yield predicted probabilities outside the interval [0,1]. This is not the case when using a model that explicitly places constraints on the range of the predicted probabilities, such as (multinomial) logistic regression. In case of, for example, multinomial logistic regression, the sum of the predicted probabilities will always be 1 as multinomial logistic regression models a joint probability distribution for all outcomes. In case of the MLPM, just like with the regular “binary” LPM, there are no constraints on the range of the predicted probabilities, which implies that there is the potential for\n\\sum_{j=1}^{k} Pr(Y_{i} = k) ≠ 1\nThis obviously doesn’t make sense. The sum of predicted probabilities cannot be greater or smaller than 1. I assumed that this fundamental flaw of the “MLPM” would make it a poor classifier.\nLittle did I know…"
  },
  {
    "objectID": "posts/mlpm/index.html#importing-packages-and-simulating-data",
    "href": "posts/mlpm/index.html#importing-packages-and-simulating-data",
    "title": "Don’t use linear regression for multiclass classification - Problems with the Multinomial Linear Probability Model",
    "section": "Importing packages and simulating data",
    "text": "Importing packages and simulating data\nFirst, let’s import the required packages and define a function that simulates our data.\n#Import required packages\nimport pandas as pd\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nI specify the target to have 3 classes (0: liking Britney, 1: liking Taylor, 2: liking Cher), 1000 observations, and a total of 10 features. Furthermore, I split the data into a training and a testing data set. This is not strictly necessary in our case, but it’s always good to check if our model can generalize to unseen data.\ndef simulate_data(classes = 3):\n  X, y = make_classification(n_samples = 1000, \n                             n_features = 10, \n                             n_informative = 3, \n                             n_redundant = 7, \n                             n_classes = classes, \n                             random_state = 1)\n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)\n  return(X_train, X_test, y_train, y_test)"
  },
  {
    "objectID": "posts/mlpm/index.html#implementing-the-mlpm",
    "href": "posts/mlpm/index.html#implementing-the-mlpm",
    "title": "Don’t use linear regression for multiclass classification - Problems with the Multinomial Linear Probability Model",
    "section": "Implementing the MLPM",
    "text": "Implementing the MLPM\nNext, we implement our state-of-the-art MLPM by passing LinearRegression() from sklearn.linear_model to the OneVsRestClassifier. We, then, calculate the predicted proabilities and get the classification_report as a Pandas data frame. The classification_report here simply picks the highest predicted probability as the predicted value.\ndef mlpm(X_train, X_test, y_train, y_test):\n  \"\"\"This function fits multinomial linear probability models on the test data, \n  and gets predictions for the training data\n  \"\"\"\n  lpm = OneVsRestClassifier(LinearRegression()).fit(X_train, y_train)\n  y_pred = lpm.predict(X_test)\n  lpm_report = classification_report(y_test, y_pred, output_dict = True)\n  lpm_report = pd.DataFrame(lpm_report).transpose()\n  return lpm_report"
  },
  {
    "objectID": "posts/mlpm/index.html#implementing-relevant-baselines-multinomial-logistic-regression-and-k-nearest-neighbors",
    "href": "posts/mlpm/index.html#implementing-relevant-baselines-multinomial-logistic-regression-and-k-nearest-neighbors",
    "title": "Don’t use linear regression for multiclass classification - Problems with the Multinomial Linear Probability Model",
    "section": "Implementing relevant baselines: multinomial logistic regression and K-nearest neighbors",
    "text": "Implementing relevant baselines: multinomial logistic regression and K-nearest neighbors\nTo gauge the performance of the MLPM, I implemented two “outdated” alternatives to the MLPM (“outdated” as the MLPM is super novel, from 2022 😏), namely multinomial logistic regression and K nearest neighbors. Multinomial logistic regression is an extension of logistic regression (a parametric supervised learning algorithm), and K nearest neighbors (a non-parametric supervised learning method).\nI defined two functions for multinomial logistic regression and KNN, respecitively:\n#Create multnomial logistic regression\n\ndef multinom(X_train, X_test, y_train, y_test):\n  \"\"\"This function fits multinomial logistic regression on the test data, \n  and gets predictions for the training data\n  \"\"\"\n  multinom = OneVsRestClassifier(LogisticRegression(multi_class = \"multinomial\")).fit(X_train, y_train)\n  y_pred = multinom.predict(X_test)\n  multinom_report = classification_report(y_test, y_pred, output_dict = True)\n  multinom_report = pd.DataFrame(multinom_report).transpose()\n  return multinom_report\n\n#Create KNN Classifier\n\ndef knn(X_train, X_test, y_train, y_test, classes):\n  \"\"\"This function fits KNN on the test data, \n  and gets predictions for the training data\n  \"\"\"\n  knn = KNeighborsClassifier(n_neighbors = classes)\n  knn.fit(X_train, y_train)\n  y_pred = knn.predict(X_test)\n  knn_report = classification_report(y_test, y_pred, output_dict = True)\n  knn_report = pd.DataFrame(knn_report).transpose()\n  return knn_report"
  },
  {
    "objectID": "posts/mlpm/index.html#plotting-the-results",
    "href": "posts/mlpm/index.html#plotting-the-results",
    "title": "Don’t use linear regression for multiclass classification - Problems with the Multinomial Linear Probability Model",
    "section": "Plotting the results",
    "text": "Plotting the results\nLastly, I define a function to plot the results and wrap all previous functions in a main function:\ndef plot(lpm, log, knn):\n \"\"\"This function plots the F1 scores per class and averaged for all three models\"\"\"\n fig, ax = plt.subplots(nrows=1, ncols=1)\n fig.suptitle('F1 scores of multinomial regressions and KNN')\n \n #Set line style and line width\n ls = \"-\"\n lw = 2.5\n\n #Add lines for the 3 models\n plt.plot(knn.index[:-2], knn['f1-score'][0:4], marker = 'o', color = 'g',\n             linewidth=lw, linestyle=ls)\n plt.plot(log.index[:-2], log['f1-score'][0:4], marker = 'o', color = 'b',\n             linewidth=lw, linestyle=ls)\n plt.plot(lpm.index[:-2], lpm['f1-score'][0:4], marker = 'o', color = 'r', \n             linewidth=lw, linestyle=ls)\n\n \n #Set axes title, label, and legend\n ax.set_ylabel('F1 score')\n ax.set_xlabel('Class')\n ax.legend(('KNN', 'multinomial \\nlogistic regression', 'MLPM'))\n\n #Plot formatting\n plt.xticks(['0', '1', '2', 'accuracy'], ['Britney', 'Taylor', 'Cher', 'mean'])\n plt.ylim([0, 1])\n plt.show()\n\ndef main():\n \"\"\"This function calculates the three models and plots the results\"\"\"\n classes = 3\n X_train, X_test, y_train, y_test = simulate_data(classes = classes)\n mod1 = mlpm(X_train, X_test, y_train, y_test)\n mod2 = multinom(X_train, X_test, y_train, y_test)\n mod3 = knn(X_train, X_test, y_train, y_test, classes = classes)\n plot(mod1, mod2, mod3)\n plt.show()"
  },
  {
    "objectID": "posts/marginally-significant/index.html",
    "href": "posts/marginally-significant/index.html",
    "title": "There are no “marginally significant” p-values",
    "section": "",
    "text": "Researchers use p-values in the context of hypothesis testing to decide whether to accept or reject the null hypothesis. The routine goes as follows: before running the analysis/the hypothesis test, we decide on a significance level (or “alpha level”, typically 5% or 0.05) below which we decide to reject the null hypothesis. Next, we run the analysis/hypothesis test, and compute the p-value. If the p-value is below the significance level, we reject the null hypothesis: the p-value is “statistically significant”. If the p-value is above the significance level, we “fail to reject” the null hypothesis: the p-value is not statistically significant.\nThis is illustrated in this nifty pic. If we get a small p-value we are usually happy (no matter how close it is to 0.05), if we get a big p-value we cry (and wonder if our paper will ever get published). But what about p-values that are “hovering” just above 0.05?\n\nIn many published papers you will find such a “third kind” of p-value, which is commonly called the “marginally significant” p-value. Alternative names include p-values that are “trending towards significance”, “approaching significance”, “hovering just above the significance level”, or “almost significant”.\nSuch “marginally significant” p-values, however, simply do not exist. In other words, labeling some p-values as “marginally significant” is a statistical mistake (which is why I use this term between ” “). Let me tell you why."
  },
  {
    "objectID": "posts/marginally-significant/index.html#reason-1-mixing-up-frameworks-for-interpreting-p-values",
    "href": "posts/marginally-significant/index.html#reason-1-mixing-up-frameworks-for-interpreting-p-values",
    "title": "There are no “marginally significant” p-values",
    "section": "Reason 1: Mixing up frameworks for interpreting p-values",
    "text": "Reason 1: Mixing up frameworks for interpreting p-values\nHistorically, there were two mutually exclusive frameworks to interpret p-values: Fisher’s and Neyman’s & Pearson’s.\nFisher intended for p-values to be interpreted as a continuous measure of the strength of the evidence against the null hypothesis. Under this framework, a p-value of 0.20 is viewed as weaker evidence against the null than a p-value of 0.02.\nNeyman & Pearson advocated to classify p-values as either significant or not significant, based on a pre-defined threshold ( \\alpha ). If p&lt; \\alpha, we reject the null hypothesis. If p &gt; \\alpha, we fail to reject the null hypothesis.\nOur current way of interpreting p-values is a mix of the two: on the one hand, we apply a decision rule (like in the Neyman-Pearson framework), on the other hand, we use stars to indicate how small a p-value is: one star for p &lt; .05, two stars for 0.05 &lt; p &lt; .01, and two stars for p &lt; .001. Some R functions, like lm(), even use a dot ‘.’ for p-values between 0.1 and 0.05:\nsummary(lm(formula = y ~ x1 + x2 + x3, data = data))\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.95251    0.03297   89.55   &lt;2e-16 ***\nx1          -1.50722    0.01847  -81.58   &lt;2e-16 ***\nx2           0.81344    0.01808   44.99   &lt;2e-16 ***\nx3          -0.82236    0.03950  -20.82   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\nResearchers might be confused about the two paradigms and interpret p-values in a more Fisherian fashion, and not evaluate them vis-a-vis a decision threshold. Or rather: researchers do acknowledge that p-values should be below the chosen \\alpha-level in order to be interpreted as significant, but if the p-value is - in their subjective perception - “close enough” to 0.05, it is “practically” significant, by only a small margin. A.k.a., the p-value is “marginally significant”.\nUnfortunately, such an interpretation makes no sense. None of the three frameworks allow for the existence of “marginally significant” p-values. In Fisher’s framework, there is no hard boundary at which p-values automatically become “significant”. Per Fisher you would conclude that p=0.049 and p=0.051 constitute practically equivalent evidence against the null, but you would not call either “significant” per se. In Neyman-Pearson and our current way of interpreting p-values, we are interested in whether p&lt; \\alpha. There is no third kind of p-value that hovers just above our chosen \\alpha-level."
  },
  {
    "objectID": "posts/marginally-significant/index.html#reason-2-academic-publish-or-perish-environment-and-researchers-disappointment",
    "href": "posts/marginally-significant/index.html#reason-2-academic-publish-or-perish-environment-and-researchers-disappointment",
    "title": "There are no “marginally significant” p-values",
    "section": "Reason 2: Academic publish or perish environment and researchers’ disappointment",
    "text": "Reason 2: Academic publish or perish environment and researchers’ disappointment\nStudies in academia take a lot of time to conduct and write up, and oftentimes even longer to get published. Imagine you are a researcher who has spent the last two years (and potentially thousands, if not hundreds of thousands, of euros of tax payers’ money) on a study. You collected the data, ran the analysis, and the p-value of your hypothesis test is 0.06. The horror! How are you gonna get this published??? You know that journals are way more likely to publish papers that have significant p-values than papers reporting “null findings”! You were so sure that you were gonna get a significant p-value!\nWhat do you do?\nThe best option is, of course, to report the true outcome: the p-value is above the chosen \\alpha level, so based on the data that you have, there is insufficient evidence to reject the null. No biggie. Alternatively, you could resort to questionable or downright unethical research practices like p-hacking. Or you could label your result “marginally significant”, implying that “in the right light, if I had a somewhat bigger sample size, if the stars aligned, this would be significant, I swear!”\nTo be clear, the second option is terrible, don’t do it! The third option is somewhat better, but you should still not do it. For once, you are bending the rules: you shouldn’t willy-nilly change your \\alpha-level after running the analysis just because you don’t like the result (that’s not science). Second, you don’t actually know if you would get a “more significant” p-value if you had more data/if the sample size were bigger."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "You can always drop me a line at hello@christianfang.eu"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Published\n\n2024\nLenny Stoeldraijer, Coen van Duin, Peteke Feijten, Christian Fang, and Niels Kooiman. (2024) \"Huishoudensprognose 2024-2070: bijna 10 miljoen huishoudens verwacht in 2070.\" Statistische Trends\n        \n        Published\n    \nChristian Fang, Anne-Rigt Poortman, and M.D. (Anne) Brons. (2024) \"Parents’ perceptions of cohesion in diverse stepfamilies.\" Family Relations\n        \n        Published\n    \nChristian Fang. (2024) \"Family life in postdivorce families.\" Utrecht University\n        \n        Published\n    \n2023\nChristian Fang and Ulrike Zartler. (2023) \"Adolescents’ experiences with ambiguity in postdivorce stepfamilies.\" Journal of Marriage and Family\n        \n        Published\n    \nChristian Fang and Anne-Rigt Poortman. (2023) \"Whom do married and divorced parents consider kin?.\" European Societies\n        \n        Published\n    \nChristian Fang, Qixiang Fang, and Dong Nguyen. (2023) \"Epicurus at SemEval-2023 Task 4: Improving Prediction of Human Values behind Arguments by Leveraging Their Definitions.\" Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\n        \n        Preprint\n     \n        \n        Published\n    \n2022\nChristian Fang. (2022) \"De Bel, Vera (2020). The ripple effect in family networks: Relational structures and well-being in divorced and non-divorced families.(166 p.). Groningen: University of Groningen. ISBN 978-94-034-2725-6..\" Mens & Maatschappij\n        \n        Published\n    \nChristian Fang, Anne-Rigt Poortman, and Tanja van der Lippe. (2022) \"Family rituals in postdivorce families: The role of family structure and relationship quality for parents’ and stepparents’ attendance at children’s birthdays.\" Journal of Family Research\n        \n        Preprint\n     \n        \n        Published\n    \n2021\nChristian Fang and Ilse van Liempt. (2021) \"‘We prefer our Dutch’: International students’ housing experiences in the Netherlands.\" Housing Studies\n        \n        Published\n    \n2019\nChristian Fang and Isa van der Wielen. (2019) \"Stigma op achterstandswijken–onjuist, onwaar en oneerlijk.\" AGORA: Magazine voor Sociaalruimtelijke vraagstukken\n        \n        Published\n    \n\nWorking Papers / Non-archival"
  },
  {
    "objectID": "posts/interactions/index.html",
    "href": "posts/interactions/index.html",
    "title": "Visualizing Interaction Effects in Logistic Regression and Linear Probability Model",
    "section": "",
    "text": "Many social science studies are interested in investigating interaction effects, or: how the relationship between a predictor (x_{1}) and an outcome (y) depends on a second predictor (x_2). x_2 is, in this context, sometimes also called a “moderator”, as it moderates or influences the relationship between x_1 and x_2.\nSuch interaction effects are pretty straightforward in linear regression models, but they tend to be more complicated in a logistic regression model (i.e., when the dependent variable of interest is binary, a.k.a., “zero or one”). There are multiple complications in logistic regression: the sign of the logistic regression coefficient is sometimes misleading (i.e., the coefficient might be negative, but the interaction might be positive for some values of the predictors in the model), and the interaction term in general depends on the values of the variables in the interaction.\nOne solution to making sense of interactions in logistic regression is to use visualizations, a.k.a., plotting the interactions. In this post, I discuss some examples of logistic regression interactions. I consider interactions between:\n\na dummy variable (0 or 1) and a continuous predictor,\na dummy variable and another dummy variable, and\na continuous predictor and another continuous predictor.\n\nAdditionally, as many people might be tempted to just use linear regression instead for ease of interpretation (i.e., fitting a so-called “linear probability model”), I will illustrate why doing so is usually not a good idea.\nNote: This post is not code-heavy as my other posts, you can check out the code reproducing all plots on my GitHub."
  },
  {
    "objectID": "posts/interactions/index.html#dummy---continuous-interaction",
    "href": "posts/interactions/index.html#dummy---continuous-interaction",
    "title": "Visualizing Interaction Effects in Logistic Regression and Linear Probability Model",
    "section": "Dummy - Continuous Interaction",
    "text": "Dummy - Continuous Interaction\nLet’s consider an example. We want to estimate how the probability of renting an apartment (as opposed to being a homeowner; y) depends on wealth (x_1) and how the effect of wealth (x_1) on renting (y) differs between rural and urban areas (x_2, where 0=rural, 1=urban).\nWe simulate some data and run two models: a logistic regression model and a linear probability model. Then, we plot the relationship between the predicted probability of renting and two lines for wealth: effect of wealth in rural areas and effect of wealth in urban areas. The two plots are shown below:\n\n\nLogistic Regression\nThe first thing we notice about the logistic regression plot is that both lines are nonlinear and S-shaped. This is due to the “logit link” or “logistic transformation” that happens when you fit a logistic regression model. That transformation constrains the predicted probabilities to the [0,1] interval.\nWe see that both in rural and urban regions, more wealth means a lower probability of renting (so, a higher probability of being a homeowner). The relationship between wealth and renting, furthermore, differs a bit between rural and urban regions. In urban regions, the probability of renting is generally higher, but an increase in wealth lowers the probability of renting more substantially than in rural areas. When wealth is about 1.5, the relationship reverses: at high levels of wealth, the probability of renting is higher in rural areas.\nThe S-shaped curve also makes substantive sense: if you are a millionaire already, a, say, €10k increase in wealth is probability not going to reduce your probability of renting as much as when you only have €100k.\n\n\nLinear Probability Model (LPM)\nThe right part of the figure shows the interaction between wealth and renting in an LPM. As the LPM is just linear regression, it imposes a linear relationship between the independent variables and the dependent variable. Therefore, we see no S-shaped curve here, but two straight lines.\nLet us first look at the range of the y-axis. The y-axis, which shows the predicted probabilities of renting, ranges from 1.5 to -0.25. Obviously, this doesn’t make any sense: the probability of renting cannot be 150% or -25%. This is the principal, incorrigible, flaw of the LPM. In almost any LPM that contains continuous variables, you will get nonsensical predicted probabilities.\nBut does the LPM interpretation of the interaction at least match that one of the logistic regression model? Not quite. On the whole, LPM does get it sort of right: the relationship between wealth and renting is negative; urban areas have a higher probability of renting than rural areas, but the difference between urban and rural regions shrinks as wealth increases. However, the lines do not cross: LPM apparently does not pick up that the relationship reverses at large values of wealth.\nI played around a bit with the setup of the simulation, and when I increased the simulated coefficient for the interaction (from -0.5 to -0.8), LPM did eventually show that the relationship reversed, but at a completely different level of wealth than in the logistic regression model (2.2 instead of 1.25)."
  },
  {
    "objectID": "posts/interactions/index.html#dummy---dummy-interaction",
    "href": "posts/interactions/index.html#dummy---dummy-interaction",
    "title": "Visualizing Interaction Effects in Logistic Regression and Linear Probability Model",
    "section": "Dummy - Dummy Interaction",
    "text": "Dummy - Dummy Interaction\nWhen we interact two dummy variables, we want to see how the predicted probabilities of a discrete variable depend on the values of another discrete variable. For example, does the relationship between being unemployed (0=working, 1=unemployed) depend on whether someone lives in a rural or urban region?\nWe, again, run a logistic regression and a linear probability model to investigate this. The plot is given below:\n\nThe interpretation here is straight forward and does not differ between logistic regression and the LPM. Unemployed people have a very high probability of being renters, and the probability of renting is higher in urban than in rural regions. People who are not unemployed have a lower probability of renting, especially those in rural regions.\nInterestingly enough, the results from the LPM are exactly the same as those from logistic regression, and there do not seem to be any nonsensical predicted probabilities!"
  },
  {
    "objectID": "posts/interactions/index.html#continuous---continuous-interaction",
    "href": "posts/interactions/index.html#continuous---continuous-interaction",
    "title": "Visualizing Interaction Effects in Logistic Regression and Linear Probability Model",
    "section": "Continuous - Continuous Interaction",
    "text": "Continuous - Continuous Interaction\nLastly, let’s see what happens when we interact two continuous variables. Suppose we want to test how the relationship between wealth and renting depends on how happy people are (x_2).\nWe, again, run a logistic regression and an LPM:\n\n\nLogistic Regression\nThe left part of the plot shows that, in logistic regression, the relationship between wealth and renting is, in general, negative: the wealthier someone is, the less likely that someone will be a renter. This relationship depends, however, on how happy someone is. Up until a value of about “1” on wealth, happier people have a higher probability of renting than people who are less happy. At a value of “1” on wealth, the relationship reverses: now, happy people have a lower probability of renting than sad people.\n\n\nLinear Probability Model\nThe LPM gets the overall pattern somewhat OK: the gap between the probabilities of happy and less happy people being renters decreases as wealth increases. Unlike logistic regression, the LPM fails to show that the relationship reverses at a value of about “1” on wealth. Instead, the LPM shows the lines to converge around 4.8 on “wealth”. Furthermore, the range of predicted probabilities makes no sense: the y-axis ranges from a bit more than 2(!) to about -0.4. This corresponds to a 200% or -40% probability of renting. Yikes!"
  },
  {
    "objectID": "posts/simulations/index.html",
    "href": "posts/simulations/index.html",
    "title": "Learn how to run a basic simulation study in R",
    "section": "",
    "text": "You probably know that statistical and machine learning methods are are based on a lot of math. So. much. math. For example, we can prove mathematically that OLS estimates \\hat\\beta - provided all of the Gauss-Markov assumptions are met - will in the long run converge to the true population parameters \\beta.\nBut what if you do not want to do the math and you still want to test if a given statistical model “works” in a specific situation? Enter simulation studies. In a simulation study, we effectively choose the truth (the so called ground truth) and can test if estimate from our statistical model of choice in the long run converge to that ground truth. We do this by simulating data following from the ground truth we specify, run the model on it, and examine how close the model estimates are to our ground truth. We repeat this process usually thousands of times to get a good idea of how a model behaves “on average” or “in the long run”.\nThis sounds complicated, but is thankfully very straightforward to implement. If you are like me, soon enough you might find yourself running simulations every time you encounter a new statistical model because you just want to see what the heck it does :)"
  },
  {
    "objectID": "posts/simulations/index.html#defining-the-data",
    "href": "posts/simulations/index.html#defining-the-data",
    "title": "Learn how to run a basic simulation study in R",
    "section": "Defining the data",
    "text": "Defining the data\nFor this example, let’s suppose that we want to simulate data based on the following equation:\nincome = 50000 + 10000 * data engineer + e(\\mu=1, \\sigma=0)\nIn other words, we assume that the outcome (income) is defined as a linear combination of the intercept term (50), 10 * data engineer, and normally distributed error term. This ground truth can be interpreted in the following ways: data scientists on average make 50k a year, and data engineers make 10k more than data scientists. Why do we add an error term here? Several reasons, but the main one is that simulations are supposed to approximate “real life”, and in real life there’s always some part of the variation of the outcome we can’t explain due to completely random processes.\nWe translate this ground truth into the following R code:\nmake_data = function(sample_size = 10000, \n                          intercept = 50, \n                          beta_1 = 10) {\n  data_engineer &lt;- rbinom(n = sample_size, size = 1, prob = 0.5)\n  income &lt;- intercept + beta_1 * data_engineer + rnorm(n=sample_size)\n  data.frame(income, data_engineer)}\nrbinom here is used to randomly generate a dummy variable, with the probability of an observation being 0 (=data scientist) or 1 (=data engineer) being .5. This does not mean that in every single data set we use there will be “50% data engineers” and “50% data scientists” (there values might be 51% and 49%), but that this is true on average in the long run. rnorm simply draws a random number from the normal distribution, and simulates our error term."
  },
  {
    "objectID": "posts/simulations/index.html#running-the-models",
    "href": "posts/simulations/index.html#running-the-models",
    "title": "Learn how to run a basic simulation study in R",
    "section": "Running the models",
    "text": "Running the models\nThere are many ways of specifying the code for running the models. My preferred way is to do it in a for loop - I’m that basic.\nLet’s first define the key parameters of the simulation. nSims defines the number of repetitions (here set to 1000). We specify a random seed using set.seed so that you get the exact same results as me when running this simulation. We set up two empty containers (linearreg and ttest) to store our model estimates.\nThe simulation itself is executed in the for loop. For every iteration in nSims, we generate a data set using make_data() we defined above, run a linear regression and store its coefficient in linearreg, and run a t-test and store the mean difference in ttest.\nGrab a cup of coffee while this is running, it might take a while depending on your computer.\n#Set number of simulations\nnSims &lt;- 1000\n#Set random seed for reproducibility\nset.seed(42)\n#Set up vectors for storing results\nlinearreg &lt;- numeric(nSims)\nttest &lt;- numeric(nSims)\n\n#Run the simulation\nfor(i in 1:nSims){\n  #Simulate the data\n  data &lt;- make_data()\n  \n  #Estimate linear regression and store coefficient\n  linear &lt;- lm('income ~ data_engineer', data=data)\n  linearreg[i] &lt;- coef(linear)[2]\n    \n  #Estimate t-test and store result\n  t_test &lt;- t.test(income ~ data_engineer, data=data, var.equal=FALSE)\n  ttest[i] &lt;- (t_test[[\"estimate\"]][[\"mean in group 1\"]]-t_test[[\"estimate\"]][[\"mean in group 0\"]])\n}"
  },
  {
    "objectID": "posts/simulations/index.html#examining-simulation-results",
    "href": "posts/simulations/index.html#examining-simulation-results",
    "title": "Learn how to run a basic simulation study in R",
    "section": "Examining simulation results",
    "text": "Examining simulation results\nNow that we have our simulation results, it’s time to examine them and draw a conclusion: can we just use a t-test instead of a linear regression when comparing two means (in the specific situation - ground truth - we defined above)?\nLet’s first examine the means of all estimates from linearreg and ttest. If the t-test is as good as the linear regression, its mean difference should be extremely close to the linear regression coefficient, given that we simulated so many data sets.\nmean(linearreg)\n[1] 9.999933\n\nmean(ttest)\n[1] 9.999933\nAs we see see, the means are identical. How about the histograms of estimates?\nhist(linearreg, main=\"Histogram of estimates from linear regression\", xlab=(\"regression coefficient\"))\nhist(ttest, main=\"Histogram of estimates from t-test\", xlab=(\"mean difference\"))\n\n\n\n\n\n\nRegression\n\n\n\n\n\n\n\nt-test\n\n\n\n\n\nThey, too, look completely identical and both look like a normal distribution. If we increase the size of nSims, they histogram will eventually look exactly like a normal distribution.\nWhat does this tell us? Yes, if we have a simple model like this (with only an intercept and one dummy variable and normally distributed errors), linear regression and an independent sample t-test recover the exact same mean difference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi there!",
    "section": "",
    "text": "I am a statistical researcher in the demography team of Statistics Netherlands (CBS). Before joining Statistics Netherlands, I obtained a PhD at the Department of Sociology, Utrecht University. In my free time, I do CrossFit 4-5 times a week and care deeply about politics. Some of my favorite blogs are Reality’s Last Stand, Citation Needed and Helen Joyce.\nOn this website, you will find my publications, nerdy statistics blogs, and contact information.\nNote that all opinions expressed on this website are strictly my own and are in no way supposed to reflect those of Statistics Netherlands."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Statistics Netherlands/Centraal Bureau voor de Statistiek (CBS) Sept 2023 – Present\n\nProducing (register-based) statistics on family formation and dissolution, with a special focus on unmarried cohabitation and same-sex families\nCollaborating with academic researchers on scientific publications\nMethodological and process innovations, e.g., transitioning from SPSS to R or benchmarking different statistical approaches and models\n\n\n\n\nDepartment of Sociology, Utrecht University Sept 2019 - Aug 2023\n\nQuantitative research on postdivorce families, cumulating in my doctoral dissertation (see below)\nTeaching statistics workgroups"
  },
  {
    "objectID": "cv.html#employment",
    "href": "cv.html#employment",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Statistics Netherlands/Centraal Bureau voor de Statistiek (CBS) Sept 2023 – Present\n\nProducing (register-based) statistics on family formation and dissolution, with a special focus on unmarried cohabitation and same-sex families\nCollaborating with academic researchers on scientific publications\nMethodological and process innovations, e.g., transitioning from SPSS to R or benchmarking different statistical approaches and models\n\n\n\n\nDepartment of Sociology, Utrecht University Sept 2019 - Aug 2023\n\nQuantitative research on postdivorce families, cumulating in my doctoral dissertation (see below)\nTeaching statistics workgroups"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "Education",
    "text": "Education\n\nDoctor of Philosophy, Sociology\nDepartment of Sociology, Utrecht University, The Netherlands awarded March 1st, 2024\n\nDissertation: Family life in postdivorce families\nSupervisors: prof. dr. Anne-Rigt Poortman, prof. dr. Tanja van der Lippe\n\n\n\nM.Sc, Urban and Economic Geography\nFaculty of Geosciences, Utrecht University, The Netherlands Sept 2017 - July 2019\n\nThesis: ‘We prefer our Dutch’: International students’ housing experiences in the Netherlands\n\n\n\nB.Sc., Geography\nFaculty of Geosciences, Ruhr University Bochum, Germany Oct 2014 - July 2017"
  }
]