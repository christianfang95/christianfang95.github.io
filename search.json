[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Let’s Talk About Sex (and Demography)\n\n\n\nstatistics\n\ndemography\n\n\n\nGender self-ID may affirm individual identities, but undermines the interpretability of sex-disaggregated statistics that are indispensable for demographic forecasting, healthcare planning, and equality monitoring.\n\n\n\n\n\nSep 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLearn how to run a basic simulation study in R\n\n\n\nstatistics\n\ndataviz\n\nR\n\n\n\nSimulation studies are extremely valuable tools to understand how statistical models behave. Learn how to run a simulation study comparing linear regression and t-tests in R!\n\n\n\n\n\nMar 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThere are no “marginally significant” p-values\n\n\n\nstatistics\n\np-values\n\n\n\nSome researchers claim that some non-significant p-values are actually “marginally significant”. Find out why they’re wrong.\n\n\n\n\n\nNov 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t use linear regression for multiclass classification - Problems with the Multinomial Linear Probability Model\n\n\n\nstatistics\n\nLPM\n\nregression\n\nPython\n\n\n\nLearn how to use linear regression for multiclass classification, and why doing sort of works but is not a good idea.\n\n\n\n\n\nNov 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Interaction Effects in Logistic Regression and Linear Probability Model\n\n\n\nstatistics\n\ndataviz\n\nR\n\n\n\nLearn how to interpret plots of interactions in logistic regression and why using LPM instead is a bad idea.\n\n\n\n\n\nOct 31, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/mlpm/index.html",
    "href": "posts/mlpm/index.html",
    "title": "Don’t use linear regression for multiclass classification - Problems with the Multinomial Linear Probability Model",
    "section": "",
    "text": "In the second edition of “An Introduction to Statistical Learning”, James, Witten, Hastie, and Tibshirani outline why it is a bad idea to use linear regression for a multiclass classification task (see pg. 131). In short, the linear regression coefficients and predicted probabilities are dependent on the ordering of the classes.\nSuppose we want to predict whether someone likes Britney Spears, Taylor Swift, or Cher. If we define the outcome/target variable as follows:\n  Y =\n  \\begin{cases}\n    0: Britney \\\\\n    1: Taylor \\\\\n    2: Cher\n  \\end{cases} \nand run a linear regression, the coefficients for the predictors mean something completely different compared to if we simply reorder the classes, for example like this:\n  Y =\n  \\begin{cases}\n    0: Taylor \\\\\n    1: Cher \\\\\n    2: Britney\n  \\end{cases} \nFurthermore, linear regression assumes that the difference between the classes is equally big (which makes absolutely no sense when you talk about a qualitative target variable).\nOn the other hand, James et al. also mention that using linear regression for a binary classification task is generally not all that problematic, though the authors rightly conclude that such an approach is still undesirable for many reasons, for example because you can get nonsensical predicted probabilities such as -0.2 or 1.5. What James et al. don’t mention is that using linear regression on a binary dependent variable is considered a valid approach by (mostly/only) economists and sociologists, who refer to such a model as a “linear probability model” or LPM. I have written about the LPM before and I generally do not think that the LPM makes sense or should be used. However, I hadn’t really thought of using LPMs for classification tasks.\nThat gave me an idea: can’t we simply decompose a multiclass classification problem into a series of LPMs? Would it work, as in: would the prediction performance be at least reasonably high to warrant the use of such a model? After all, estimating a series of LPMs is sort of the same thing as approximating a multinomial logistic regression model by fitting a series of binary logistic regressions.\nNote: This model is meant to be a joke (literally). As you will see from my conclusion, I do not recommend using the MLPM as a classification algorithm. You should also not use the LPM… Also note that the example of liking either Britney, Taylor, or Cher is completely ridiculous. Of course you should like all of them!\nYou can find the code also on GitHub.\n\n\nThe “multinomial linear probability model” is not actually a proper statistical model, it is just a series separate “binary” LPMs put together. I am not 100% sure about the mathematical formula for the MLPM (since I just made it up and the model doesn’t really make sense), but I guess it should be something like this (correct me if I’m wrong!):\nPr(Y_{i} = k) = \\alpha + \\sum_{j=1}^{k} \\beta_{k}X_{i}\nIn other words, if we have k classes to predict, we fit k separate binary LPMs. For example, if we have three classes, [0, 1, 2] (e.g., liking Taylor, Britney, or Cher), then we would simply fit three LPMs:\nPr(Y_{i} = 0) = \\alpha + \\sum \\beta_{i}X_{i}\nPr(Y_{i} = 1) = \\alpha + \\sum \\beta_{i}X_{i}\nPr(Y_{i} = 2) = \\alpha + \\sum \\beta_{i}X_{i}\nThis gives us three predicted probabilities:\n\nthe probability of the observation being in class 0 (as opposed to 1 and 2) (e.g., liking Britney vs. Taylor and Cher)\nthe probability of the observation being in class 1 (as opposed to 0 and 2) (e.g., liking Taylor vs. Britney and Cher)\nthe probability of the observation being in class 2 (as opposed to 0 and 1) (e.g., liking Cher vs. Taylor and Britney)\n\nMy intuition told me that this is a deeply weird approach. As described above, the problem with the LPM is that it can yield predicted probabilities outside the interval [0,1]. This is not the case when using a model that explicitly places constraints on the range of the predicted probabilities, such as (multinomial) logistic regression. In case of, for example, multinomial logistic regression, the sum of the predicted probabilities will always be 1 as multinomial logistic regression models a joint probability distribution for all outcomes. In case of the MLPM, just like with the regular “binary” LPM, there are no constraints on the range of the predicted probabilities, which implies that there is the potential for\n\\sum_{j=1}^{k} Pr(Y_{i} = k) ≠ 1\nThis obviously doesn’t make sense. The sum of predicted probabilities cannot be greater or smaller than 1. I assumed that this fundamental flaw of the “MLPM” would make it a poor classifier.\nLittle did I know…"
  },
  {
    "objectID": "posts/mlpm/index.html#inventing-anna-the-multinomial-linear-probability-model-mlpm",
    "href": "posts/mlpm/index.html#inventing-anna-the-multinomial-linear-probability-model-mlpm",
    "title": "Don’t use linear regression for multiclass classification - Problems with the Multinomial Linear Probability Model",
    "section": "",
    "text": "The “multinomial linear probability model” is not actually a proper statistical model, it is just a series separate “binary” LPMs put together. I am not 100% sure about the mathematical formula for the MLPM (since I just made it up and the model doesn’t really make sense), but I guess it should be something like this (correct me if I’m wrong!):\nPr(Y_{i} = k) = \\alpha + \\sum_{j=1}^{k} \\beta_{k}X_{i}\nIn other words, if we have k classes to predict, we fit k separate binary LPMs. For example, if we have three classes, [0, 1, 2] (e.g., liking Taylor, Britney, or Cher), then we would simply fit three LPMs:\nPr(Y_{i} = 0) = \\alpha + \\sum \\beta_{i}X_{i}\nPr(Y_{i} = 1) = \\alpha + \\sum \\beta_{i}X_{i}\nPr(Y_{i} = 2) = \\alpha + \\sum \\beta_{i}X_{i}\nThis gives us three predicted probabilities:\n\nthe probability of the observation being in class 0 (as opposed to 1 and 2) (e.g., liking Britney vs. Taylor and Cher)\nthe probability of the observation being in class 1 (as opposed to 0 and 2) (e.g., liking Taylor vs. Britney and Cher)\nthe probability of the observation being in class 2 (as opposed to 0 and 1) (e.g., liking Cher vs. Taylor and Britney)\n\nMy intuition told me that this is a deeply weird approach. As described above, the problem with the LPM is that it can yield predicted probabilities outside the interval [0,1]. This is not the case when using a model that explicitly places constraints on the range of the predicted probabilities, such as (multinomial) logistic regression. In case of, for example, multinomial logistic regression, the sum of the predicted probabilities will always be 1 as multinomial logistic regression models a joint probability distribution for all outcomes. In case of the MLPM, just like with the regular “binary” LPM, there are no constraints on the range of the predicted probabilities, which implies that there is the potential for\n\\sum_{j=1}^{k} Pr(Y_{i} = k) ≠ 1\nThis obviously doesn’t make sense. The sum of predicted probabilities cannot be greater or smaller than 1. I assumed that this fundamental flaw of the “MLPM” would make it a poor classifier.\nLittle did I know…"
  },
  {
    "objectID": "posts/mlpm/index.html#importing-packages-and-simulating-data",
    "href": "posts/mlpm/index.html#importing-packages-and-simulating-data",
    "title": "Don’t use linear regression for multiclass classification - Problems with the Multinomial Linear Probability Model",
    "section": "Importing packages and simulating data",
    "text": "Importing packages and simulating data\nFirst, let’s import the required packages and define a function that simulates our data.\n#Import required packages\nimport pandas as pd\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nI specify the target to have 3 classes (0: liking Britney, 1: liking Taylor, 2: liking Cher), 1000 observations, and a total of 10 features. Furthermore, I split the data into a training and a testing data set. This is not strictly necessary in our case, but it’s always good to check if our model can generalize to unseen data.\ndef simulate_data(classes = 3):\n  X, y = make_classification(n_samples = 1000, \n                             n_features = 10, \n                             n_informative = 3, \n                             n_redundant = 7, \n                             n_classes = classes, \n                             random_state = 1)\n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)\n  return(X_train, X_test, y_train, y_test)"
  },
  {
    "objectID": "posts/mlpm/index.html#implementing-the-mlpm",
    "href": "posts/mlpm/index.html#implementing-the-mlpm",
    "title": "Don’t use linear regression for multiclass classification - Problems with the Multinomial Linear Probability Model",
    "section": "Implementing the MLPM",
    "text": "Implementing the MLPM\nNext, we implement our state-of-the-art MLPM by passing LinearRegression() from sklearn.linear_model to the OneVsRestClassifier. We, then, calculate the predicted proabilities and get the classification_report as a Pandas data frame. The classification_report here simply picks the highest predicted probability as the predicted value.\ndef mlpm(X_train, X_test, y_train, y_test):\n  \"\"\"This function fits multinomial linear probability models on the test data, \n  and gets predictions for the training data\n  \"\"\"\n  lpm = OneVsRestClassifier(LinearRegression()).fit(X_train, y_train)\n  y_pred = lpm.predict(X_test)\n  lpm_report = classification_report(y_test, y_pred, output_dict = True)\n  lpm_report = pd.DataFrame(lpm_report).transpose()\n  return lpm_report"
  },
  {
    "objectID": "posts/mlpm/index.html#implementing-relevant-baselines-multinomial-logistic-regression-and-k-nearest-neighbors",
    "href": "posts/mlpm/index.html#implementing-relevant-baselines-multinomial-logistic-regression-and-k-nearest-neighbors",
    "title": "Don’t use linear regression for multiclass classification - Problems with the Multinomial Linear Probability Model",
    "section": "Implementing relevant baselines: multinomial logistic regression and K-nearest neighbors",
    "text": "Implementing relevant baselines: multinomial logistic regression and K-nearest neighbors\nTo gauge the performance of the MLPM, I implemented two “outdated” alternatives to the MLPM (“outdated” as the MLPM is super novel, from 2022 😏), namely multinomial logistic regression and K nearest neighbors. Multinomial logistic regression is an extension of logistic regression (a parametric supervised learning algorithm), and K nearest neighbors (a non-parametric supervised learning method).\nI defined two functions for multinomial logistic regression and KNN, respecitively:\n#Create multnomial logistic regression\n\ndef multinom(X_train, X_test, y_train, y_test):\n  \"\"\"This function fits multinomial logistic regression on the test data, \n  and gets predictions for the training data\n  \"\"\"\n  multinom = OneVsRestClassifier(LogisticRegression(multi_class = \"multinomial\")).fit(X_train, y_train)\n  y_pred = multinom.predict(X_test)\n  multinom_report = classification_report(y_test, y_pred, output_dict = True)\n  multinom_report = pd.DataFrame(multinom_report).transpose()\n  return multinom_report\n\n#Create KNN Classifier\n\ndef knn(X_train, X_test, y_train, y_test, classes):\n  \"\"\"This function fits KNN on the test data, \n  and gets predictions for the training data\n  \"\"\"\n  knn = KNeighborsClassifier(n_neighbors = classes)\n  knn.fit(X_train, y_train)\n  y_pred = knn.predict(X_test)\n  knn_report = classification_report(y_test, y_pred, output_dict = True)\n  knn_report = pd.DataFrame(knn_report).transpose()\n  return knn_report"
  },
  {
    "objectID": "posts/mlpm/index.html#plotting-the-results",
    "href": "posts/mlpm/index.html#plotting-the-results",
    "title": "Don’t use linear regression for multiclass classification - Problems with the Multinomial Linear Probability Model",
    "section": "Plotting the results",
    "text": "Plotting the results\nLastly, I define a function to plot the results and wrap all previous functions in a main function:\ndef plot(lpm, log, knn):\n \"\"\"This function plots the F1 scores per class and averaged for all three models\"\"\"\n fig, ax = plt.subplots(nrows=1, ncols=1)\n fig.suptitle('F1 scores of multinomial regressions and KNN')\n \n #Set line style and line width\n ls = \"-\"\n lw = 2.5\n\n #Add lines for the 3 models\n plt.plot(knn.index[:-2], knn['f1-score'][0:4], marker = 'o', color = 'g',\n             linewidth=lw, linestyle=ls)\n plt.plot(log.index[:-2], log['f1-score'][0:4], marker = 'o', color = 'b',\n             linewidth=lw, linestyle=ls)\n plt.plot(lpm.index[:-2], lpm['f1-score'][0:4], marker = 'o', color = 'r', \n             linewidth=lw, linestyle=ls)\n\n \n #Set axes title, label, and legend\n ax.set_ylabel('F1 score')\n ax.set_xlabel('Class')\n ax.legend(('KNN', 'multinomial \\nlogistic regression', 'MLPM'))\n\n #Plot formatting\n plt.xticks(['0', '1', '2', 'accuracy'], ['Britney', 'Taylor', 'Cher', 'mean'])\n plt.ylim([0, 1])\n plt.show()\n\ndef main():\n \"\"\"This function calculates the three models and plots the results\"\"\"\n classes = 3\n X_train, X_test, y_train, y_test = simulate_data(classes = classes)\n mod1 = mlpm(X_train, X_test, y_train, y_test)\n mod2 = multinom(X_train, X_test, y_train, y_test)\n mod3 = knn(X_train, X_test, y_train, y_test, classes = classes)\n plot(mod1, mod2, mod3)\n plt.show()"
  },
  {
    "objectID": "posts/simulations/index.html",
    "href": "posts/simulations/index.html",
    "title": "Learn how to run a basic simulation study in R",
    "section": "",
    "text": "You probably know that statistical and machine learning methods are are based on a lot of math. So. much. math. For example, we can prove mathematically that OLS estimates \\hat\\beta - provided all of the Gauss-Markov assumptions are met - will in the long run converge to the true population parameters \\beta.\nBut what if you do not want to do the math and you still want to test if a given statistical model “works” in a specific situation? Enter simulation studies. In a simulation study, we effectively choose the truth (the so called ground truth) and can test if estimate from our statistical model of choice in the long run converge to that ground truth. We do this by simulating data following from the ground truth we specify, run the model on it, and examine how close the model estimates are to our ground truth. We repeat this process usually thousands of times to get a good idea of how a model behaves “on average” or “in the long run”.\nThis sounds complicated, but is thankfully very straightforward to implement. If you are like me, soon enough you might find yourself running simulations every time you encounter a new statistical model because you just want to see what the heck it does :)"
  },
  {
    "objectID": "posts/simulations/index.html#defining-the-data",
    "href": "posts/simulations/index.html#defining-the-data",
    "title": "Learn how to run a basic simulation study in R",
    "section": "Defining the data",
    "text": "Defining the data\nFor this example, let’s suppose that we want to simulate data based on the following equation:\nincome = 50000 + 10000 * data engineer + e(\\mu=1, \\sigma=0)\nIn other words, we assume that the outcome (income) is defined as a linear combination of the intercept term (50), 10 * data engineer, and normally distributed error term. This ground truth can be interpreted in the following ways: data scientists on average make 50k a year, and data engineers make 10k more than data scientists. Why do we add an error term here? Several reasons, but the main one is that simulations are supposed to approximate “real life”, and in real life there’s always some part of the variation of the outcome we can’t explain due to completely random processes.\nWe translate this ground truth into the following R code:\nmake_data = function(sample_size = 10000, \n                          intercept = 50, \n                          beta_1 = 10) {\n  data_engineer &lt;- rbinom(n = sample_size, size = 1, prob = 0.5)\n  income &lt;- intercept + beta_1 * data_engineer + rnorm(n=sample_size)\n  data.frame(income, data_engineer)}\nrbinom here is used to randomly generate a dummy variable, with the probability of an observation being 0 (=data scientist) or 1 (=data engineer) being .5. This does not mean that in every single data set we use there will be “50% data engineers” and “50% data scientists” (there values might be 51% and 49%), but that this is true on average in the long run. rnorm simply draws a random number from the normal distribution, and simulates our error term."
  },
  {
    "objectID": "posts/simulations/index.html#running-the-models",
    "href": "posts/simulations/index.html#running-the-models",
    "title": "Learn how to run a basic simulation study in R",
    "section": "Running the models",
    "text": "Running the models\nThere are many ways of specifying the code for running the models. My preferred way is to do it in a for loop - I’m that basic.\nLet’s first define the key parameters of the simulation. nSims defines the number of repetitions (here set to 1000). We specify a random seed using set.seed so that you get the exact same results as me when running this simulation. We set up two empty containers (linearreg and ttest) to store our model estimates.\nThe simulation itself is executed in the for loop. For every iteration in nSims, we generate a data set using make_data() we defined above, run a linear regression and store its coefficient in linearreg, and run a t-test and store the mean difference in ttest.\nGrab a cup of coffee while this is running, it might take a while depending on your computer.\n#Set number of simulations\nnSims &lt;- 1000\n#Set random seed for reproducibility\nset.seed(42)\n#Set up vectors for storing results\nlinearreg &lt;- numeric(nSims)\nttest &lt;- numeric(nSims)\n\n#Run the simulation\nfor(i in 1:nSims){\n  #Simulate the data\n  data &lt;- make_data()\n  \n  #Estimate linear regression and store coefficient\n  linear &lt;- lm('income ~ data_engineer', data=data)\n  linearreg[i] &lt;- coef(linear)[2]\n    \n  #Estimate t-test and store result\n  t_test &lt;- t.test(income ~ data_engineer, data=data, var.equal=FALSE)\n  ttest[i] &lt;- (t_test[[\"estimate\"]][[\"mean in group 1\"]]-t_test[[\"estimate\"]][[\"mean in group 0\"]])\n}"
  },
  {
    "objectID": "posts/simulations/index.html#examining-simulation-results",
    "href": "posts/simulations/index.html#examining-simulation-results",
    "title": "Learn how to run a basic simulation study in R",
    "section": "Examining simulation results",
    "text": "Examining simulation results\nNow that we have our simulation results, it’s time to examine them and draw a conclusion: can we just use a t-test instead of a linear regression when comparing two means (in the specific situation - ground truth - we defined above)?\nLet’s first examine the means of all estimates from linearreg and ttest. If the t-test is as good as the linear regression, its mean difference should be extremely close to the linear regression coefficient, given that we simulated so many data sets.\nmean(linearreg)\n[1] 9.999933\n\nmean(ttest)\n[1] 9.999933\nAs we see see, the means are identical. How about the histograms of estimates?\nhist(linearreg, main=\"Histogram of estimates from linear regression\", xlab=(\"regression coefficient\"))\nhist(ttest, main=\"Histogram of estimates from t-test\", xlab=(\"mean difference\"))\n\n\n\n\n\n\nRegression\n\n\n\n\n\n\n\nt-test\n\n\n\n\n\nThey, too, look completely identical and both look like a normal distribution. If we increase the size of nSims, they histogram will eventually look exactly like a normal distribution.\nWhat does this tell us? Yes, if we have a simple model like this (with only an intercept and one dummy variable and normally distributed errors), linear regression and an independent sample t-test recover the exact same mean difference."
  },
  {
    "objectID": "posts/interactions/index.html",
    "href": "posts/interactions/index.html",
    "title": "Visualizing Interaction Effects in Logistic Regression and Linear Probability Model",
    "section": "",
    "text": "Many social science studies are interested in investigating interaction effects, or: how the relationship between a predictor (x_{1}) and an outcome (y) depends on a second predictor (x_2). x_2 is, in this context, sometimes also called a “moderator”, as it moderates or influences the relationship between x_1 and x_2.\nSuch interaction effects are pretty straightforward in linear regression models, but they tend to be more complicated in a logistic regression model (i.e., when the dependent variable of interest is binary, a.k.a., “zero or one”). There are multiple complications in logistic regression: the sign of the logistic regression coefficient is sometimes misleading (i.e., the coefficient might be negative, but the interaction might be positive for some values of the predictors in the model), and the interaction term in general depends on the values of the variables in the interaction.\nOne solution to making sense of interactions in logistic regression is to use visualizations, a.k.a., plotting the interactions. In this post, I discuss some examples of logistic regression interactions. I consider interactions between:\n\na dummy variable (0 or 1) and a continuous predictor,\na dummy variable and another dummy variable, and\na continuous predictor and another continuous predictor.\n\nAdditionally, as many people might be tempted to just use linear regression instead for ease of interpretation (i.e., fitting a so-called “linear probability model”), I will illustrate why doing so is usually not a good idea.\nNote: This post is not code-heavy as my other posts, you can check out the code reproducing all plots on my GitHub."
  },
  {
    "objectID": "posts/interactions/index.html#dummy---continuous-interaction",
    "href": "posts/interactions/index.html#dummy---continuous-interaction",
    "title": "Visualizing Interaction Effects in Logistic Regression and Linear Probability Model",
    "section": "Dummy - Continuous Interaction",
    "text": "Dummy - Continuous Interaction\nLet’s consider an example. We want to estimate how the probability of renting an apartment (as opposed to being a homeowner; y) depends on wealth (x_1) and how the effect of wealth (x_1) on renting (y) differs between rural and urban areas (x_2, where 0=rural, 1=urban).\nWe simulate some data and run two models: a logistic regression model and a linear probability model. Then, we plot the relationship between the predicted probability of renting and two lines for wealth: effect of wealth in rural areas and effect of wealth in urban areas. The two plots are shown below:\n\n\nLogistic Regression\nThe first thing we notice about the logistic regression plot is that both lines are nonlinear and S-shaped. This is due to the “logit link” or “logistic transformation” that happens when you fit a logistic regression model. That transformation constrains the predicted probabilities to the [0,1] interval.\nWe see that both in rural and urban regions, more wealth means a lower probability of renting (so, a higher probability of being a homeowner). The relationship between wealth and renting, furthermore, differs a bit between rural and urban regions. In urban regions, the probability of renting is generally higher, but an increase in wealth lowers the probability of renting more substantially than in rural areas. When wealth is about 1.5, the relationship reverses: at high levels of wealth, the probability of renting is higher in rural areas.\nThe S-shaped curve also makes substantive sense: if you are a millionaire already, a, say, €10k increase in wealth is probability not going to reduce your probability of renting as much as when you only have €100k.\n\n\nLinear Probability Model (LPM)\nThe right part of the figure shows the interaction between wealth and renting in an LPM. As the LPM is just linear regression, it imposes a linear relationship between the independent variables and the dependent variable. Therefore, we see no S-shaped curve here, but two straight lines.\nLet us first look at the range of the y-axis. The y-axis, which shows the predicted probabilities of renting, ranges from 1.5 to -0.25. Obviously, this doesn’t make any sense: the probability of renting cannot be 150% or -25%. This is the principal, incorrigible, flaw of the LPM. In almost any LPM that contains continuous variables, you will get nonsensical predicted probabilities.\nBut does the LPM interpretation of the interaction at least match that one of the logistic regression model? Not quite. On the whole, LPM does get it sort of right: the relationship between wealth and renting is negative; urban areas have a higher probability of renting than rural areas, but the difference between urban and rural regions shrinks as wealth increases. However, the lines do not cross: LPM apparently does not pick up that the relationship reverses at large values of wealth.\nI played around a bit with the setup of the simulation, and when I increased the simulated coefficient for the interaction (from -0.5 to -0.8), LPM did eventually show that the relationship reversed, but at a completely different level of wealth than in the logistic regression model (2.2 instead of 1.25)."
  },
  {
    "objectID": "posts/interactions/index.html#dummy---dummy-interaction",
    "href": "posts/interactions/index.html#dummy---dummy-interaction",
    "title": "Visualizing Interaction Effects in Logistic Regression and Linear Probability Model",
    "section": "Dummy - Dummy Interaction",
    "text": "Dummy - Dummy Interaction\nWhen we interact two dummy variables, we want to see how the predicted probabilities of a discrete variable depend on the values of another discrete variable. For example, does the relationship between being unemployed (0=working, 1=unemployed) depend on whether someone lives in a rural or urban region?\nWe, again, run a logistic regression and a linear probability model to investigate this. The plot is given below:\n\nThe interpretation here is straight forward and does not differ between logistic regression and the LPM. Unemployed people have a very high probability of being renters, and the probability of renting is higher in urban than in rural regions. People who are not unemployed have a lower probability of renting, especially those in rural regions.\nInterestingly enough, the results from the LPM are exactly the same as those from logistic regression, and there do not seem to be any nonsensical predicted probabilities!"
  },
  {
    "objectID": "posts/interactions/index.html#continuous---continuous-interaction",
    "href": "posts/interactions/index.html#continuous---continuous-interaction",
    "title": "Visualizing Interaction Effects in Logistic Regression and Linear Probability Model",
    "section": "Continuous - Continuous Interaction",
    "text": "Continuous - Continuous Interaction\nLastly, let’s see what happens when we interact two continuous variables. Suppose we want to test how the relationship between wealth and renting depends on how happy people are (x_2).\nWe, again, run a logistic regression and an LPM:\n\n\nLogistic Regression\nThe left part of the plot shows that, in logistic regression, the relationship between wealth and renting is, in general, negative: the wealthier someone is, the less likely that someone will be a renter. This relationship depends, however, on how happy someone is. Up until a value of about “1” on wealth, happier people have a higher probability of renting than people who are less happy. At a value of “1” on wealth, the relationship reverses: now, happy people have a lower probability of renting than sad people.\n\n\nLinear Probability Model\nThe LPM gets the overall pattern somewhat OK: the gap between the probabilities of happy and less happy people being renters decreases as wealth increases. Unlike logistic regression, the LPM fails to show that the relationship reverses at a value of about “1” on wealth. Instead, the LPM shows the lines to converge around 4.8 on “wealth”. Furthermore, the range of predicted probabilities makes no sense: the y-axis ranges from a bit more than 2(!) to about -0.4. This corresponds to a 200% or -40% probability of renting. Yikes!"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Published\n\n2024\nLenny Stoeldraijer, Coen van Duin, Peteke Feijten, Christian Fang, and Niels Kooiman. (2024) \"Huishoudensprognose 2024-2070: bijna 10 miljoen huishoudens verwacht in 2070.\" Statistische Trends\n        \n        Published\n    \nChristian Fang, Anne-Rigt Poortman, and M.D. (Anne) Brons. (2024) \"Parents’ perceptions of cohesion in diverse stepfamilies.\" Family Relations\n        \n        Published\n    \nChristian Fang. (2024) \"Family life in postdivorce families.\" Utrecht University\n        \n        Published\n    \n2023\nChristian Fang and Ulrike Zartler. (2023) \"Adolescents’ experiences with ambiguity in postdivorce stepfamilies.\" Journal of Marriage and Family\n        \n        Published\n    \nChristian Fang and Anne-Rigt Poortman. (2023) \"Whom do married and divorced parents consider kin?.\" European Societies\n        \n        Published\n    \nChristian Fang, Qixiang Fang, and Dong Nguyen. (2023) \"Epicurus at SemEval-2023 Task 4: Improving Prediction of Human Values behind Arguments by Leveraging Their Definitions.\" Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\n        \n        Preprint\n     \n        \n        Published\n    \n2022\nChristian Fang. (2022) \"De Bel, Vera (2020). The ripple effect in family networks: Relational structures and well-being in divorced and non-divorced families.(166 p.). Groningen: University of Groningen. ISBN 978-94-034-2725-6..\" Mens & Maatschappij\n        \n        Published\n    \nChristian Fang, Anne-Rigt Poortman, and Tanja van der Lippe. (2022) \"Family rituals in postdivorce families: The role of family structure and relationship quality for parents’ and stepparents’ attendance at children’s birthdays.\" Journal of Family Research\n        \n        Preprint\n     \n        \n        Published\n    \n2021\nChristian Fang and Ilse van Liempt. (2021) \"‘We prefer our Dutch’: International students’ housing experiences in the Netherlands.\" Housing Studies\n        \n        Published\n    \n2019\nChristian Fang and Isa van der Wielen. (2019) \"Stigma op achterstandswijken–onjuist, onwaar en oneerlijk.\" AGORA: Magazine voor Sociaalruimtelijke vraagstukken\n        \n        Published\n    \n\nWorking Papers / Non-archival"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "You can always drop me a line at hello@christianfang.eu"
  },
  {
    "objectID": "posts/marginally-significant/index.html",
    "href": "posts/marginally-significant/index.html",
    "title": "There are no “marginally significant” p-values",
    "section": "",
    "text": "Researchers use p-values in the context of hypothesis testing to decide whether to accept or reject the null hypothesis. The routine goes as follows: before running the analysis/the hypothesis test, we decide on a significance level (or “alpha level”, typically 5% or 0.05) below which we decide to reject the null hypothesis. Next, we run the analysis/hypothesis test, and compute the p-value. If the p-value is below the significance level, we reject the null hypothesis: the p-value is “statistically significant”. If the p-value is above the significance level, we “fail to reject” the null hypothesis: the p-value is not statistically significant.\nThis is illustrated in this nifty pic. If we get a small p-value we are usually happy (no matter how close it is to 0.05), if we get a big p-value we cry (and wonder if our paper will ever get published). But what about p-values that are “hovering” just above 0.05?\n\nIn many published papers you will find such a “third kind” of p-value, which is commonly called the “marginally significant” p-value. Alternative names include p-values that are “trending towards significance”, “approaching significance”, “hovering just above the significance level”, or “almost significant”.\nSuch “marginally significant” p-values, however, simply do not exist. In other words, labeling some p-values as “marginally significant” is a statistical mistake (which is why I use this term between ” “). Let me tell you why."
  },
  {
    "objectID": "posts/marginally-significant/index.html#reason-1-mixing-up-frameworks-for-interpreting-p-values",
    "href": "posts/marginally-significant/index.html#reason-1-mixing-up-frameworks-for-interpreting-p-values",
    "title": "There are no “marginally significant” p-values",
    "section": "Reason 1: Mixing up frameworks for interpreting p-values",
    "text": "Reason 1: Mixing up frameworks for interpreting p-values\nHistorically, there were two mutually exclusive frameworks to interpret p-values: Fisher’s and Neyman’s & Pearson’s.\nFisher intended for p-values to be interpreted as a continuous measure of the strength of the evidence against the null hypothesis. Under this framework, a p-value of 0.20 is viewed as weaker evidence against the null than a p-value of 0.02.\nNeyman & Pearson advocated to classify p-values as either significant or not significant, based on a pre-defined threshold ( \\alpha ). If p&lt; \\alpha, we reject the null hypothesis. If p &gt; \\alpha, we fail to reject the null hypothesis.\nOur current way of interpreting p-values is a mix of the two: on the one hand, we apply a decision rule (like in the Neyman-Pearson framework), on the other hand, we use stars to indicate how small a p-value is: one star for p &lt; .05, two stars for 0.05 &lt; p &lt; .01, and two stars for p &lt; .001. Some R functions, like lm(), even use a dot ‘.’ for p-values between 0.1 and 0.05:\nsummary(lm(formula = y ~ x1 + x2 + x3, data = data))\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.95251    0.03297   89.55   &lt;2e-16 ***\nx1          -1.50722    0.01847  -81.58   &lt;2e-16 ***\nx2           0.81344    0.01808   44.99   &lt;2e-16 ***\nx3          -0.82236    0.03950  -20.82   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\nResearchers might be confused about the two paradigms and interpret p-values in a more Fisherian fashion, and not evaluate them vis-a-vis a decision threshold. Or rather: researchers do acknowledge that p-values should be below the chosen \\alpha-level in order to be interpreted as significant, but if the p-value is - in their subjective perception - “close enough” to 0.05, it is “practically” significant, by only a small margin. A.k.a., the p-value is “marginally significant”.\nUnfortunately, such an interpretation makes no sense. None of the three frameworks allow for the existence of “marginally significant” p-values. In Fisher’s framework, there is no hard boundary at which p-values automatically become “significant”. Per Fisher you would conclude that p=0.049 and p=0.051 constitute practically equivalent evidence against the null, but you would not call either “significant” per se. In Neyman-Pearson and our current way of interpreting p-values, we are interested in whether p&lt; \\alpha. There is no third kind of p-value that hovers just above our chosen \\alpha-level."
  },
  {
    "objectID": "posts/marginally-significant/index.html#reason-2-academic-publish-or-perish-environment-and-researchers-disappointment",
    "href": "posts/marginally-significant/index.html#reason-2-academic-publish-or-perish-environment-and-researchers-disappointment",
    "title": "There are no “marginally significant” p-values",
    "section": "Reason 2: Academic publish or perish environment and researchers’ disappointment",
    "text": "Reason 2: Academic publish or perish environment and researchers’ disappointment\nStudies in academia take a lot of time to conduct and write up, and oftentimes even longer to get published. Imagine you are a researcher who has spent the last two years (and potentially thousands, if not hundreds of thousands, of euros of tax payers’ money) on a study. You collected the data, ran the analysis, and the p-value of your hypothesis test is 0.06. The horror! How are you gonna get this published??? You know that journals are way more likely to publish papers that have significant p-values than papers reporting “null findings”! You were so sure that you were gonna get a significant p-value!\nWhat do you do?\nThe best option is, of course, to report the true outcome: the p-value is above the chosen \\alpha level, so based on the data that you have, there is insufficient evidence to reject the null. No biggie. Alternatively, you could resort to questionable or downright unethical research practices like p-hacking. Or you could label your result “marginally significant”, implying that “in the right light, if I had a somewhat bigger sample size, if the stars aligned, this would be significant, I swear!”\nTo be clear, the second option is terrible, don’t do it! The third option is somewhat better, but you should still not do it. For once, you are bending the rules: you shouldn’t willy-nilly change your \\alpha-level after running the analysis just because you don’t like the result (that’s not science). Second, you don’t actually know if you would get a “more significant” p-value if you had more data/if the sample size were bigger."
  },
  {
    "objectID": "posts/gender-self-id/index.html",
    "href": "posts/gender-self-id/index.html",
    "title": "Let’s Talk About Sex (and Demography)",
    "section": "",
    "text": "Note: This article reflects my personal opinions, not those of my employer.\nGermany introduced its gender self-identification act (Selbstbestimmungsgesetz) in late 2024. The law allows individuals to change their sex marker once per year through a simple declaration at the local courthouse. The Netherlands planned a similar legal change, though the draft was retracted due to lack of support in the current parliament. The stated intention is to respect and affirm individuals’ gender identities. However, from the perspective of official statistics and demography, such reforms introduce substantial challenges.\nSex-disaggregated data form the foundation of demographic analysis, fertility and mortality statistics, healthcare planning, labor market monitoring, and international comparability. If “sex” is redefined as a mutable and subjective administrative category, these statistics risk losing their meaning altogether. The core concern is not ideological but conceptual: many of the most important demographic and health indicators are meaningful only if sex reflects biology. If this link is severed, statistics become uninterpretable, forecasts unreliable, and policy decisions distorted."
  },
  {
    "objectID": "posts/gender-self-id/index.html#sex",
    "href": "posts/gender-self-id/index.html#sex",
    "title": "Let’s Talk About Sex (and Demography)",
    "section": "1.1 Sex",
    "text": "1.1 Sex\nIn biology, sex is defined by the reproductive role of an organism—specifically, the type of gamete it produces (or is organized to produce). Males produce small gametes (sperm), while females produce large gametes (ova). This anisogamy-based definition applies universally to mammals, including humans, and establishes sex as a binary trait (Bhargava et al. 2021; Dawkins 1976).\nCritics often argue that such a definition “erases” intersex individuals, framing them as evidence of a spectrum rather than a binary (Fausto-Sterling 1993, 2000). Yet biological consensus is clear: intersex conditions represent variations within the male–female binary, not a third reproductive class. While some individuals have atypical chromosomal, hormonal, or anatomical features, every known human falls within the framework of male or female. No documented case exists of a person simultaneously producing both viable sperm and ova (Sax 2002). For statistical purposes, sex can therefore be consistently operationalized as a binary variable, even if clinical expertise is sometimes required to record it."
  },
  {
    "objectID": "posts/gender-self-id/index.html#gender-and-self-id",
    "href": "posts/gender-self-id/index.html#gender-and-self-id",
    "title": "Let’s Talk About Sex (and Demography)",
    "section": "1.2 Gender and Self-ID",
    "text": "1.2 Gender and Self-ID\nBy contrast, “gender” typically refers to an individual’s internal sense of identity and social role, which may or may not align with biological sex (West and Zimmerman 1987; Butler 1990). Queer and trans theorists argue that gender is socially constructed, fluid, and historically contingent (Spade 2011; Judith Halberstam 2005).\nSelf-identification policies operationalize this view by allowing individuals to change their legal sex marker based solely on self-declaration, without medical or psychological evaluation. Historically, legal recognition of a sex change required lengthy medical and legal procedures, reflecting what Dean Spade calls the “administrative violence” of gatekeeping (Spade 2011). Under self-ID, this requirement is removed, and the sex marker becomes a reflection of personal declaration rather than biological reality."
  },
  {
    "objectID": "posts/gender-self-id/index.html#from-rare-exceptions-to-systemic-instability",
    "href": "posts/gender-self-id/index.html#from-rare-exceptions-to-systemic-instability",
    "title": "Let’s Talk About Sex (and Demography)",
    "section": "1.3 From Rare Exceptions to Systemic Instability",
    "text": "1.3 From Rare Exceptions to Systemic Instability\nEven under the previous medical/legal regime, the demographic categories of male or female were not perfectly “pure.” Individuals who successfully transitioned legally were reclassified in registers, creating some distortion of male and female categories. But because such cases were rare—limited to those who underwent extensive medical and legal procedures—the statistical impact was contained.\nSelf-ID fundamentally alters this balance. What was once rare and exceptional becomes administratively simple and potentially widespread. Survey evidence from the Netherlands suggests that nearly one million people—around 6% of the adult population—would consider registering an “X” marker under a self-ID regime (Rutgers 2023). This is not a marginal distortion but a systemic break in statistical continuity."
  },
  {
    "objectID": "posts/gender-self-id/index.html#loss-of-semantic-coherence",
    "href": "posts/gender-self-id/index.html#loss-of-semantic-coherence",
    "title": "Let’s Talk About Sex (and Demography)",
    "section": "2.1 Loss of Semantic Coherence",
    "text": "2.1 Loss of Semantic Coherence\nThe most immediate consequence of self-ID regimes is that certain statistics lose their substantive meaning. If legal sex replaces biological sex in administrative data, categories such as “male fertility” or “non-binary fertility” will appear. These are conceptually incoherent from a demographic perspective, because fertility and mortality processes are tied to reproductive biology, not self-declared identity.\n\n2.1.1 Fertility and the Female Sex Class\nFormally, consider the total fertility rate (TFR): \n\\text{TFR} = \\sum_a \\frac{B_a}{W_a} \\cdot k,\n\nwhere B_a is the number of births to women of age a, W_a is the number of women of age a, and k is a scaling constant (usually 5 for 5-year age groups). The denominator W_a is meaningful only if “women” refers to individuals biologically capable of childbearing (i.e., females with a uterus).\nIf the denominator includes males who are legally reclassified as female, it introduces individuals who are categorically incapable of pregnancy. If, conversely, biologically female individuals are recorded as male or non-binary, their births inflate the numerator but not the denominator. In both cases, the statistic ceases to measure what it was designed to measure: female fertility.\nIt is sometimes argued that fertility statistics are already flawed because not all women can give birth. Yet demographic indicators operate at the group level, not the individual level. The TFR does not assume that every woman will reproduce, but that fertility is restricted to the female sex class. Including infertile or post-reproductive women is not an error but part of the design: the measure captures the average number of births per woman, given the distribution of fertility across the population. This is conceptually distinct from including males, who are never members of the reproductive sex class.\n\n\n2.1.2 Fertility, Family Diversity, and Forecasting\nAnother critique is that fertility measures reinforce heteronormative assumptions, especially since same-sex couples can also become parents (Valentine 2007). This, however, confuses athways to parenthood with fertility as a biological process. Adoption, step-parenthood, and surrogacy are important social realities, but they do not change the fact that every birth requires a uterus. The very existence of surrogacy debates highlights this: male couples can only become biological parents through the involvement of a female gestational surrogate. Fertility rates therefore remain indispensable as measures of biological reproduction and population replacement.\nForecasting also does not assume heterosexual, monogamous marriages. It tracks births and cohort sizes, not family forms. Whether a child is raised by heterosexual parents, same-sex parents, or a single parent is irrelevant for the measure itself. What matters is the number of children born, because this determines future school enrollments, labor supply, and pension obligations. Family structure can—and should—be studied in its own right, but it is not a substitute for fertility as a demographic variable.\n\n\n2.1.3 Implications for Planning\nThe importance of fertility statistics is not abstract. They underpin population forecasts, which guide planning for schools, housing, pensions, and healthcare capacity (Lutz, Sanderson, and Scherbov 2001). If fertility rates are distorted or rendered uninterpretable, projections of the working-age population, dependency ratios, and fiscal sustainability all become unreliable. Governments would effectively be forced to plan blind. Fertility statistics are therefore not relics of a “cis-heteronormative worldview,” but practical instruments of governance."
  },
  {
    "objectID": "posts/gender-self-id/index.html#break-in-statistical-continuity",
    "href": "posts/gender-self-id/index.html#break-in-statistical-continuity",
    "title": "Let’s Talk About Sex (and Demography)",
    "section": "2.2 Break in Statistical Continuity",
    "text": "2.2 Break in Statistical Continuity\nIf sex is redefined as a self-declared category, the underlying concept shifts from a stable biological attribute to a fluid social identity. This undermines:\n\nTime series comparability: Longitudinal measures such as fertility, mortality, and life expectancy become non-comparable if the meaning of “male” and “female” changes.\nCross-country comparability: International statistical systems harmonize sex as a biological variable (United Nations 2019; Eurostat 2015; OECD 2022). Divergence reduces comparability.\nCategory stability: In Germany, individuals may re-register annually. A person could thus be male in one year, female in the next, and “X” thereafter—an unprecedented source of instability for demographic series."
  },
  {
    "objectID": "posts/gender-self-id/index.html#distortion-of-core-indicators",
    "href": "posts/gender-self-id/index.html#distortion-of-core-indicators",
    "title": "Let’s Talk About Sex (and Demography)",
    "section": "2.3 Distortion of Core Indicators",
    "text": "2.3 Distortion of Core Indicators\nIn healthcare, many conditions are sex-specific (e.g., prostate cancer, cervical cancer). Consider cervical cancer incidence:\n\nI = \\frac{C}{F}\n\nwhere C is the number of cases and F is the number of females at risk. If the denominator includes legally female but biologically male individuals (who lack cervices), the rate is downwardly biased. If biologically female individuals are legally recorded as male or non-binary, the rate is upwardly biased.\nThese statistics are not merely descriptive. They are used to plan how many gynecologists, obstetricians, and oncologists a healthcare system requires. Biologically male populations will never need obstetric care; biologically female populations will. If denominators are blurred, health systems risk under- or over-provision of essential services, directly affecting patient outcomes.\nIt is sometimes argued that sex-specific categories exclude transgender and non-binary patients (Feder 2022). But allocation must follow biological risk: biologically male patients are at risk of prostate cancer; biologically female patients of cervical cancer. Without this clarity, prevention and treatment misallocate scarce resources, harming patients.\nCrime statistics face similar challenges. Some argue that disaggregating crime by sex reinforces stereotypes of men as violent and women as victims. Yet criminology consistently finds stark sex differences in offending. Recognizing these patterns is essential for effective prevention—for example, interventions targeted at male youth violence. Replacing biological sex with legal sex obscures these differences, leading to less effective policies and weaker protection for vulnerable groups."
  },
  {
    "objectID": "posts/gender-self-id/index.html#impact-on-statistical-models",
    "href": "posts/gender-self-id/index.html#impact-on-statistical-models",
    "title": "Let’s Talk About Sex (and Demography)",
    "section": "2.4 Impact on Statistical Models",
    "text": "2.4 Impact on Statistical Models\nCategory instability also undermines econometric models that assume time-invariant characteristics such as sex. Two examples illustrate the problem.\n\n2.4.1 Fixed Effects Models\nIn a standard individual fixed-effects model, time-invariant traits such as sex are absorbed into the unit-specific intercept \\alpha_i and therefore drop out:\n\ny_{it} = \\alpha_i + \\tau_t + \\gamma'X_{it} + \\varepsilon_{it}.\n\nHere, y_{it} is the outcome for person i in year t, \\tau_t are year effects, X_{it} are time-varying covariates, and \\alpha_i captures all stable characteristics of individual i, including sex. The purpose of the within estimator is to estimate causal effects of time-varying regressors net of sex and other stable factors.\nIf “sex” becomes mutable under self-ID, however, it no longer functions as a stable trait. Instead, it enters the model as a time-varying covariate:\n\ny_{it} = \\alpha_i + \\tau_t + \\beta\\,\\text{Female}_{it}^{legal} + \\gamma'X_{it} + \\varepsilon_{it}.\n\nIn this specification, \\beta no longer measures the effect of being biologically female. It captures the effect of switching one’s administrative marker — a fundamentally different parameter, subject to measurement error and endogeneity (e.g., if switching is correlated with shocks in employment or health).\nOne might argue that the problem could be solved by defining sex at baseline (\\text{Female}_i^{0}) and then introducing an indicator for whether an individual has switched during the panel (\\text{Switch}_{it}):\n\ny_{it} = \\alpha_i + \\tau_t + \\beta\\,\\text{Female}_i^{0} + \\delta\\,\\text{Switch}_{it} + \\gamma'X_{it} + \\varepsilon_{it}.\n\nWhile statistically feasible, this approach does not resolve the conceptual issues:\n\nConceptual mismatch. \\delta measures the association between legal reclassification and the outcome, not sex differences. This is not the estimand of interest in official statistics.\n\nEndogeneity. Switching is unlikely to be random and may be correlated with shocks in y_{it}, making \\delta difficult to interpret.\n\nPolicy relevance. Policymakers seek to understand sex-based inequalities; \\delta instead captures the consequences of administrative reclassification.\n\nThus, although baseline stratification and switch indicators can be coded, they shift the estimand away from biology and towards administrative behavior, which is not the purpose of sex-disaggregated analysis.\n\n\n2.4.2 Difference-in-Differences and Event Studies\nThe same logic applies to Difference-in-Differences (DiD) and event study designs. Typically, one estimates:\n\ny_{it} = \\alpha_i + \\tau_t + \\theta \\,(\\text{Female}_i \\times \\text{Post}_t) + \\varepsilon_{it},\n\nwhere \\theta captures the female–male differential effect of a policy change.\nIf sex is mutable, group membership becomes unstable. Individuals can enter or leave the “female” group by administrative reclassification, contaminating treatment and control groups. A natural adjustment is to define groups by baseline sex and then include a switch indicator:\n\ny_{it} = \\alpha_i + \\tau_t + \\theta \\,(\\text{Female}_i^{0} \\times \\text{Post}_t) + \\delta\\,\\text{Switch}_{it} + \\varepsilon_{it}.\n\nHere, \\theta still represents the policy effect on baseline females relative to baseline males, while \\delta captures the effect of switching categories during the observation window.\nBut as with fixed effects, this patch does not solve the deeper problems:\n\nConceptual mismatch. The question DiD is meant to answer is whether a policy narrowed or widened gaps between men and women. With mutable categories, \\delta estimates the correlation between switching and outcomes, which is not the same quantity.\n\nEndogeneity. Reclassification may occur precisely in response to the policy (e.g., if incentives differ across categories), violating the parallel trends assumption.\n\nPolicy relevance. Policymakers are interested in whether interventions reduce structural inequalities between men and women. If “women” is defined administratively and subject to strategic reclassification, the results cease to reflect those inequalities.\n\nThus, even with baseline definitions and switch indicators, the estimand changes. What the model delivers is no longer a policy effect on biologically defined groups, but an effect on administratively mutable categories. For official statistics, this undermines both interpretability and policy relevance."
  },
  {
    "objectID": "posts/gender-self-id/index.html#administrative-data-integration",
    "href": "posts/gender-self-id/index.html#administrative-data-integration",
    "title": "Let’s Talk About Sex (and Demography)",
    "section": "2.5 Administrative Data Integration",
    "text": "2.5 Administrative Data Integration\nModern statistical systems rely heavily on integrating administrative registers such as population registers, vital statistics, health records, education, labor market, and pensions. This integration makes it possible to construct longitudinal life-course data and to derive core demographic indicators. A key assumption in this process is that “sex” is a stable characteristic, much like date of birth, and can therefore be used consistently across domains.\nIf sex becomes mutable under self-ID, this assumption breaks down in several ways:\n\nDifferent sex entries across registers. A person may update their marker in the population register but still appear under their biological sex in a health register (e.g., when receiving gynecological care) or in historical education records. When the registers are linked, the same individual may appear as male in one domain and female in another. Analysts are then faced with the question: which record should take precedence?\nCoherence of life-course trajectories. Consider an individual who is recorded as female in the health register at the time of giving birth, but as male in the pension register later in life. If we study fertility, employment, and pensions across the life course, the categories no longer line up: the same person’s fertility is counted among women, while their pension accruals are tabulated among men. This undermines the interpretability of longitudinal statistics.\nDownstream bias in derived indicators. Many derived indicators (e.g., lifetime earnings gaps, fertility–employment interactions, healthy life expectancy by sex) require consistent categorization over time and across registers. If sex refers to different underlying concepts in different domains—sometimes biology, sometimes self-identification—then the resulting statistics no longer measure what they purport to measure."
  },
  {
    "objectID": "posts/gender-self-id/index.html#policy-monitoring-and-legal-obligations",
    "href": "posts/gender-self-id/index.html#policy-monitoring-and-legal-obligations",
    "title": "Let’s Talk About Sex (and Demography)",
    "section": "2.6 Policy Monitoring and Legal Obligations",
    "text": "2.6 Policy Monitoring and Legal Obligations\nEquality frameworks depend on stable sex categories. Gender pay gap monitoring, pension entitlements, and labor force participation all rely on consistent sex-disaggregated data. Quotas for board representation are another example.\nSome argue that quotas should reflect gender identity rather than sex (Butler 2004; Jack Halberstam 2018). Yet quotas exist to remedy systemic disadvantages women face over their entire careers. A late-in-life male-to-female transition does not erase decades of accumulated male advantage. Counting such individuals toward female quotas undermines both the purpose and legitimacy of such policies."
  },
  {
    "objectID": "posts/gender-self-id/index.html#international-standards",
    "href": "posts/gender-self-id/index.html#international-standards",
    "title": "Let’s Talk About Sex (and Demography)",
    "section": "2.7 International Standards",
    "text": "2.7 International Standards\nInternational statistical standards coordinated by the UN, Eurostat, and OECD require sex as a harmonized demographic variable (United Nations 2019; Eurostat 2015).\nWhile some argue that such standards will evolve, harmonization only works if concepts remain stable across countries. It is highly unlikely that all national statistical offices—including those in more conservative or Global South contexts—will redefine sex administratively. A unilateral shift to self-ID would isolate a country statistically rather than modernize it."
  },
  {
    "objectID": "posts/gender-self-id/index.html#feasibility-of-recording-gender-identity",
    "href": "posts/gender-self-id/index.html#feasibility-of-recording-gender-identity",
    "title": "Let’s Talk About Sex (and Demography)",
    "section": "2.8 Feasibility of Recording Gender Identity",
    "text": "2.8 Feasibility of Recording Gender Identity\nA further challenge is the feasibility of recording gender identity in official statistics. While the categories of “male” and “female” are stable and universally recognized, there is no consensus—even within activist or queer theory circles—on how many genders exist or how they should be defined. Some advocate for a third category (“X”), others for an open-ended approach that recognizes dozens or even infinite genders (e.g., “astrogender,” “catgender”). From the perspective of official statistics, this creates two distinct problems.\nFirst, conceptual indeterminacy. International standards depend on clear and stable categories that are comparable across time and across countries. If the number of genders is, in principle, infinite or open-ended, statistical offices cannot produce harmonized demographic series. Without standardization, time trends break down, and cross-national comparability disappears.\nSecond, statistical feasibility. Even if dozens of identity categories were formally recognized in data collection, the resulting cell sizes in official tabulations would be extremely small. Confidentiality rules in official statistics generally prohibit publication of results with very small cell counts. The consequence is that most gender-diverse categories would either be suppressed for disclosure control reasons or would be statistically unreliable. In practice, the result would be a proliferation of categories in collection, but little usable information in publication.\nThird, mismatch across data sources. Surveys may allow free-text or fine-grained gender categories (e.g., “catgender”), while registers and censuses typically constrain responses to male, female, or at most “X.” As a result, the same person may be recognized as “catgender” in a survey but recorded as “female” in a register. This inconsistency creates two problems: (1) conceptually, the categories do not align across sources, making it difficult to integrate data; and (2) politically, activists may critique official statistics for erasing identities by collapsing them into standardized categories. Either way, the coherence and legitimacy of the data are undermined.\nOne proposed solution is to add a residual “other” category. Yet this approach introduces its own problems. Activists often critique “other” as an act of othering, positioning gender-diverse people as marginal or residual. Alternative wordings such as “another gender (please specify)” or “gender diverse” may reduce this rhetorical problem, but they do not resolve the statistical one: the open-ended nature of the category still yields extremely small cell sizes and unstable comparability.\nSome national statistical agencies have experimented with a third explicit category such as “non-binary” or “X.” This has the advantage of recognition beyond the male–female binary while maintaining a finite number of categories that can be harmonized internationally. Yet even this solution is contested, as not all individuals who reject the binary identify with “non-binary” or “X.” In practice, the recording of detailed gender identities may be better suited to specialized surveys designed to capture the experiences of gender-diverse populations, while censuses, vital statistics, and international reporting require stable, limited, and statistically robust categories."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi there!",
    "section": "",
    "text": "I am a statistical researcher in the demography team of Statistics Netherlands (CBS). Before joining Statistics Netherlands, I obtained a PhD at the Department of Sociology, Utrecht University. In my free time, I do CrossFit 4-5 times a week and care deeply about politics. Some of my favorite blogs are Reality’s Last Stand, Citation Needed and Helen Joyce.\nOn this website, you will find my publications, nerdy statistics blogs, and contact information.\nNote that all opinions expressed on this website are strictly my own and are in no way supposed to reflect those of Statistics Netherlands."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Statistics Netherlands/Centraal Bureau voor de Statistiek (CBS) Sept 2023 – Present\n\nProducing (register-based) statistics on family formation and dissolution, with a special focus on unmarried cohabitation and same-sex families\nCollaborating with academic researchers on scientific publications\nMethodological and process innovations, e.g., transitioning from SPSS to R or benchmarking different statistical approaches and models\n\n\n\n\nDepartment of Sociology, Utrecht University Sept 2019 - Aug 2023\n\nQuantitative research on postdivorce families, cumulating in my doctoral dissertation (see below)\nTeaching statistics workgroups"
  },
  {
    "objectID": "cv.html#employment",
    "href": "cv.html#employment",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Statistics Netherlands/Centraal Bureau voor de Statistiek (CBS) Sept 2023 – Present\n\nProducing (register-based) statistics on family formation and dissolution, with a special focus on unmarried cohabitation and same-sex families\nCollaborating with academic researchers on scientific publications\nMethodological and process innovations, e.g., transitioning from SPSS to R or benchmarking different statistical approaches and models\n\n\n\n\nDepartment of Sociology, Utrecht University Sept 2019 - Aug 2023\n\nQuantitative research on postdivorce families, cumulating in my doctoral dissertation (see below)\nTeaching statistics workgroups"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "Education",
    "text": "Education\n\nDoctor of Philosophy, Sociology\nDepartment of Sociology, Utrecht University, The Netherlands awarded March 1st, 2024\n\nDissertation: Family life in postdivorce families\nSupervisors: prof. dr. Anne-Rigt Poortman, prof. dr. Tanja van der Lippe\n\n\n\nM.Sc, Urban and Economic Geography\nFaculty of Geosciences, Utrecht University, The Netherlands Sept 2017 - July 2019\n\nThesis: ‘We prefer our Dutch’: International students’ housing experiences in the Netherlands\n\n\n\nB.Sc., Geography\nFaculty of Geosciences, Ruhr University Bochum, Germany Oct 2014 - July 2017"
  }
]